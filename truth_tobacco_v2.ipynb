{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from typing import List, Dict, Any, Set, Tuple\n",
    "from collections import defaultdict\n",
    "import google.generativeai as genai\n",
    "import urllib3\n",
    "from urllib.parse import urlencode\n",
    "import time\n",
    "from difflib import SequenceMatcher\n",
    "import tempfile\n",
    "import os\n",
    "import re\n",
    "# Update parameters\n",
    "# Main calls Analyzer(strategies, content_store, BATCH_DOC_EVAL_V1, SING_DOC_EVAL_V1,SING_DOC_SUMMARY_V1)\n",
    "class TobaccoDocAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://solr.idl.ucsf.edu/solr/ltdl3/select\"\n",
    "        self.ocr_base = \"https://download.industrydocuments.ucsf.edu/\"\n",
    "        genai.configure(api_key='AIzaSyDl7OsvN8gB8v33BkcIzmSuflwia7YOrQk')\n",
    "        self.model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "        # Document tracking\n",
    "        self.document_frequencies = defaultdict(int)\n",
    "        self.document_store = {}  # Single source of truth for all document data\n",
    "        self.title_hash = set()  # Store normalized titles for fast duplicate checking\n",
    "\n",
    "    def _normalize_title(self, title: str) -> str:\n",
    "        \"\"\"Create a normalized version of the title for comparison\"\"\"\n",
    "        if not title:\n",
    "            return \"\"\n",
    "        # Remove punctuation, convert to lowercase, and remove extra whitespace\n",
    "        normalized = ''.join(c.lower() for c in title if c.isalnum() or c.isspace())\n",
    "        return ' '.join(normalized.split())\n",
    "\n",
    "    \n",
    "    def _add_to_document_store(self, doc: Dict[str, Any]) -> bool:\n",
    "        \"\"\"Add document to store if it's not a duplicate. Return True if added.\"\"\"\n",
    "        doc_id = doc['id']\n",
    "        title = doc.get('ti', '')\n",
    "        \n",
    "        # Skip if exact ID already exists\n",
    "        if doc_id in self.document_store:\n",
    "            return False\n",
    "            \n",
    "        # Get normalized title\n",
    "        norm_title = self._normalize_title(title)\n",
    "        if not norm_title:  # If no title, just add the doc\n",
    "            self.document_store[doc_id] = {\n",
    "                'metadata': doc,\n",
    "                'analysis': None,\n",
    "                'ocr_text': None\n",
    "            }\n",
    "            return True\n",
    "            \n",
    "        # Check for duplicate title\n",
    "        if norm_title in self.title_hash:\n",
    "            print(f\"Skipping document {doc_id} due to seen title\")\n",
    "            return False\n",
    "            \n",
    "        # Add document and its normalized title\n",
    "        self.title_hash.add(norm_title)\n",
    "        self.document_store[doc_id] = {\n",
    "            'metadata': doc,\n",
    "            'analysis': None,\n",
    "            'ocr_text': None\n",
    "        }\n",
    "        return True\n",
    "    def build_url(self, params):\n",
    "        \"\"\"Build URL manually to match exact format from website\"\"\"\n",
    "        base = self.base_url + \"?q=*:*\"\n",
    "        for fq in params.get('fq', []):\n",
    "            base += f\"&fq={requests.utils.quote(fq)}\"\n",
    "        for key, value in params.items():\n",
    "            if key != 'fq':\n",
    "                base += f\"&{key}={requests.utils.quote(str(value))}\"\n",
    "        return base\n",
    "\n",
    "    def is_public_doc(self, doc):\n",
    "        \"\"\"Check if document is public and unrestricted\"\"\"\n",
    "        availability = doc.get('availability', [])\n",
    "        return \"public\" in availability or \"no restrictions\" in availability\n",
    "\n",
    "    def get_ocr_text(self, doc_id: str) -> str:\n",
    "        \"\"\"Gets OCR text for a document\"\"\"\n",
    "        path_segment = '/'.join(list(doc_id[:4].lower()))\n",
    "        url = f\"{self.ocr_base}{path_segment}/{doc_id.lower()}/{doc_id.lower()}.ocr\"\n",
    "        try:\n",
    "            response = requests.get(url, verify=False, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                return response.text\n",
    "            return \"\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting OCR text for {doc_id}: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def generate_search_strategies(self, user_query: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Generate multiple search strategies based on the user query\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        Given this research question about tobacco documents: \"{user_query}\"\n",
    "        Generate 3 different search strategies. Each strategy should contain 2-4 key terms that would help find relevant documents.\n",
    "        Return your response in this exact JSON format with no additional text:\n",
    "        {{\n",
    "            \"strategies\": [\n",
    "                {{\n",
    "                    \"search_terms\": \"term1 term2\",\n",
    "                    \"filters\": {{}},\n",
    "                    \"rationale\": \"why this might work\"\n",
    "                }}\n",
    "            ]\n",
    "        }}\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.model.generate_content(prompt)\n",
    "            response_text = response.text.strip()\n",
    "            \n",
    "            # Try to find JSON in the response\n",
    "            import re\n",
    "            json_match = re.search(r'\\{.*\\}', response_text, re.DOTALL)\n",
    "            if json_match:\n",
    "                strategies = json.loads(json_match.group())\n",
    "                print(\"\\nGenerated search strategies:\")\n",
    "                for idx, strat in enumerate(strategies['strategies'], 1):\n",
    "                    print(f\"\\nStrategy {idx}:\")\n",
    "                    print(f\"Terms: {strat['search_terms']}\")\n",
    "                    print(f\"Filters: {strat['filters']}\")\n",
    "                return strategies['strategies']\n",
    "            else:\n",
    "                return self._fallback_search_strategy(user_query)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating search strategies: {e}\")\n",
    "            return self._fallback_search_strategy(user_query)\n",
    "\n",
    "    def _fallback_search_strategy(self, query: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Generate basic search strategies from query terms\"\"\"\n",
    "        print(\"\\nFalling back to query text parsing\")\n",
    "        query_terms = query.lower()\n",
    "        # Extract any quoted phrases or word combinations\n",
    "        terms = re.findall(r'\"([^\"]*)\"|\\b\\w+\\s+\\w+\\b|\\b\\w+\\b', query_terms)\n",
    "        \n",
    "        strategies = []\n",
    "        if len(terms) >= 2:\n",
    "            strategies.append({\n",
    "                \"search_terms\": f\"{terms[0]} AND {terms[1]}\",\n",
    "                \"filters\": {},\n",
    "                \"rationale\": \"Primary terms combination\"\n",
    "            })\n",
    "        \n",
    "        if len(terms) >= 3:\n",
    "            strategies.append({\n",
    "                \"search_terms\": f\"{terms[1]} AND {terms[2]}\",\n",
    "                \"filters\": {},\n",
    "                \"rationale\": \"Secondary terms combination\"\n",
    "            })\n",
    "        \n",
    "        all_terms = \" AND \".join(terms[:4])\n",
    "        strategies.append({\n",
    "            \"search_terms\": all_terms,\n",
    "            \"filters\": {},\n",
    "            \"rationale\": \"Combined key terms\"\n",
    "        })\n",
    "        \n",
    "        return strategies\n",
    "\n",
    "    def execute_search(self, strategy: Dict[str, Any], max_results: int = 50) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Execute a single search strategy\"\"\"\n",
    "        params = {\n",
    "            'q': strategy['search_terms'],\n",
    "            'fq': ['availability:public'],\n",
    "            'wt': 'json',\n",
    "            'rows': str(max_results),\n",
    "            'sort': 'score desc',\n",
    "            'fl': 'id,au,ti,bn,dd,dt,availability,pg,attach,access,artifact'\n",
    "        }\n",
    "        \n",
    "        # Add strategy filters\n",
    "        for field, value in strategy['filters'].items():\n",
    "            params['fq'].append(f'{field}:{value}')\n",
    "\n",
    "        try:\n",
    "            response = requests.get(self.base_url, params=params, verify=False)\n",
    "            if response.status_code == 200:\n",
    "                return [doc for doc in response.json()['response']['docs'] \n",
    "                       if self.is_public_doc(doc)]\n",
    "        except Exception as e:\n",
    "            print(f\"Error executing search: {e}\")\n",
    "        return []\n",
    "\n",
    "    def analyze_topic(self, user_query: str, max_iterations: int = 3) -> Dict[str, Any]:\n",
    "        \"\"\"Main analysis pipeline\"\"\"\n",
    "        print(f\"\\nStarting analysis for: {user_query}\")\n",
    "        \n",
    "        # Reset tracking for new analysis\n",
    "        self.document_frequencies.clear()\n",
    "        self.document_store.clear()\n",
    "        self.title_hash.clear()  # Clear title hash\n",
    "        \n",
    "        # Phase 1: Initial Search\n",
    "        strategies = self.generate_search_strategies(user_query)\n",
    "        \n",
    "        for strategy in strategies:\n",
    "            print(f\"\\nExecuting strategy: {strategy['search_terms']}\")\n",
    "            docs = self.execute_search(strategy)\n",
    "            if docs:\n",
    "                new_docs = []\n",
    "                for doc in docs:\n",
    "                    doc_id = doc['id']\n",
    "                    self.document_frequencies[doc_id] += 1\n",
    "                    if self._add_to_document_store(doc):  # Use new method\n",
    "                        new_docs.append(doc)\n",
    "                \n",
    "                # Process only new documents in batches\n",
    "                if new_docs:\n",
    "                    for i in range(0, len(new_docs), 5):\n",
    "                        batch = new_docs[i:i+5]\n",
    "                        self._process_document_batch(batch, user_query)\n",
    "            time.sleep(2)  # Rate limiting\n",
    "        \n",
    "        # Phase 2: Expansion\n",
    "        iteration = 0\n",
    "        while iteration < max_iterations:\n",
    "            print(f\"\\nExpansion iteration {iteration + 1}\")\n",
    "            \n",
    "            # Sort documents by weighted score\n",
    "            sorted_docs = self._get_sorted_documents()[:15]\n",
    "            \n",
    "            new_docs_count = 0\n",
    "            for doc_id, _ in sorted_docs:\n",
    "                before_docs, target, after_docs = self.find_surrounding_docs(doc_id)\n",
    "                \n",
    "                # Process new documents found\n",
    "                new_docs = []\n",
    "                for doc in before_docs + after_docs:\n",
    "                    doc_id = doc['id']\n",
    "                    self.document_frequencies[doc_id] += 1\n",
    "                    if self._add_to_document_store(doc):  # Use new method\n",
    "                        new_docs.append(doc)\n",
    "                        new_docs_count += 1\n",
    "                \n",
    "                # Analyze new documents\n",
    "                if new_docs:\n",
    "                    self._process_document_batch(new_docs, user_query)\n",
    "                time.sleep(1)\n",
    "            \n",
    "            print(f\"Found {new_docs_count} new documents\")\n",
    "            print(f\"Total unique documents: {len(self.document_store)}\")\n",
    "            \n",
    "            # Check for saturation\n",
    "            if new_docs_count < 3:\n",
    "                print(\"Reached saturation\")\n",
    "                break\n",
    "            \n",
    "            iteration += 1\n",
    "        \n",
    "        # Prepare final results\n",
    "        final_results = self._prepare_final_results()\n",
    "        \n",
    "        # Get top docs for final analysis\n",
    "        top_docs = [self.document_store[doc_id]['metadata'] \n",
    "                for doc_id in list(final_results.keys())[:10]]\n",
    "        \n",
    "        final_analysis = self.analyze_with_gemini(top_docs, user_query)\n",
    "        \n",
    "        return {\n",
    "            'raw_results': final_results,\n",
    "            'final_analysis': final_analysis if top_docs else \"No documents found\",\n",
    "            'document_count': len(final_results),\n",
    "            'total_documents_seen': len(self.document_store),\n",
    "            'frequency_statistics': dict(self.document_frequencies)\n",
    "        }\n",
    "\n",
    "    def _process_document_batch(self, docs: List[Dict[str, Any]], query_context: str):\n",
    "        \"\"\"Process a batch of new documents\"\"\"\n",
    "        # Get OCR text for batch\n",
    "        for doc in docs:\n",
    "            doc_id = doc['id']\n",
    "            if not self.document_store[doc_id]['ocr_text']:\n",
    "                self.document_store[doc_id]['ocr_text'] = self.get_ocr_text(doc_id)\n",
    "        \n",
    "        # Analyze batch\n",
    "        analysis = self.analyze_document_batch(docs, query_context)\n",
    "        \n",
    "        # Store analysis results\n",
    "        for doc_id, doc_analysis in analysis.items():\n",
    "            if doc_id in self.document_store:\n",
    "                self.document_store[doc_id]['analysis'] = doc_analysis\n",
    "\n",
    "    def analyze_document_batch(self, docs: List[Dict[str, Any]], query_context: str) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze documents in batches with fallback to individual processing for problematic batches\"\"\"\n",
    "        if not docs:\n",
    "            return {}\n",
    "        \n",
    "        BATCH_SIZE = 5\n",
    "        batch_results = {}\n",
    "        \n",
    "        # Define the JSON example template separately\n",
    "        EXAMPLE_JSON = (\n",
    "            '{\\n'\n",
    "            '    \"doc_id1\": {\\n'\n",
    "            '        \"score\": 7,\\n'\n",
    "            '        \"entities\": {\\n'\n",
    "            '            \"people\": [\"name1\", \"name2\"],\\n'\n",
    "            '            \"projects\": [\"project1\"],\\n'\n",
    "            '            \"products\": [\"product1\"],\\n'\n",
    "            '            \"terms\": [\"term1\"],\\n'\n",
    "            '            \"dates\": [\"date1\"]\\n'\n",
    "            '        }\\n'\n",
    "            '    }\\n'\n",
    "            '}'\n",
    "        )\n",
    "        \n",
    "        # First try batch processing\n",
    "        for i in range(0, len(docs), BATCH_SIZE):\n",
    "            batch = docs[i:i+BATCH_SIZE]\n",
    "            doc_texts = []\n",
    "            batch_ids = []\n",
    "            \n",
    "            for doc in batch:\n",
    "                doc_id = doc['id']\n",
    "                title = doc.get('ti', 'No title')\n",
    "                batch_ids.append(doc_id)\n",
    "                doc_texts.append(\n",
    "                    f\"Document ID: {doc_id}\\n\"\n",
    "                    f\"Title: {title}\\n\"\n",
    "                    f\"Content:\\n{self.document_store[doc_id]['ocr_text'][:3000]}...\"\n",
    "                )\n",
    "\n",
    "            prompt = (\n",
    "                f\"Research context: {query_context}\\n\\n\"\n",
    "                \"Analyze these tobacco industry documents. For each document, provide:\\n\"\n",
    "                \"1. A relevance score (0-10)\\n\"\n",
    "                \"2. Key entities found (people, projects, products, terms, dates)\\n\\n\"\n",
    "                f\"Return only a JSON object like this example:\\n{EXAMPLE_JSON}\\n\\n\"\n",
    "                \"Documents to analyze:\\n\\n\"\n",
    "                f\"{chr(10) + '---' + chr(10)}\".join(doc_texts)\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                response = self.model.generate_content(prompt)\n",
    "                response_text = response.text.strip()\n",
    "                \n",
    "                # Try to find JSON in the response\n",
    "                json_match = re.search(r'\\{.*\\}', response_text, re.DOTALL)\n",
    "                if json_match:\n",
    "                    analysis = json.loads(json_match.group())\n",
    "                    print(f\"\\nScores for batch:\")\n",
    "                    for doc_id, details in analysis.items():\n",
    "                        print(f\"Document {doc_id}: Score {details['score']}/10\")\n",
    "                    batch_results.update(analysis)\n",
    "                    continue  # Move to next batch if successful\n",
    "                \n",
    "            except ValueError as e:\n",
    "                if \"reciting from copyrighted material\" in str(e):\n",
    "                    print(f\"\\n⚠️ Copyright detection in batch {i//BATCH_SIZE + 1} - processing documents individually\")\n",
    "                    # Process problematic batch one by one\n",
    "                    for doc in batch:\n",
    "                        doc_id = doc['id']\n",
    "                        try:\n",
    "                            individual_result = self._analyze_single_document(doc, query_context)\n",
    "                            batch_results.update(individual_result)\n",
    "                        except Exception as doc_error:\n",
    "                            print(f\"Error with document {doc_id}: {doc_error}\")\n",
    "                            batch_results.update(self._create_basic_analysis([doc]))\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\nError in batch {i//BATCH_SIZE + 1}: {e}\")\n",
    "                # Fallback to basic analysis for the entire batch\n",
    "                batch_results.update(self._create_basic_analysis(batch))\n",
    "            \n",
    "            time.sleep(0.5)  # Rate limiting between batches\n",
    "        \n",
    "        return batch_results\n",
    "\n",
    "    def _analyze_single_document(self, doc: Dict[str, Any], query_context: str) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze a single document when batch processing fails\"\"\"\n",
    "        doc_id = doc['id']\n",
    "        title = doc.get('ti', 'No title')\n",
    "        ocr_text = self.document_store[doc_id]['ocr_text'][:3000] if self.document_store[doc_id]['ocr_text'] else ''\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        Research context: {query_context}\n",
    "\n",
    "        Analyze this tobacco industry document. Instead of directly quoting, summarize:\n",
    "        1. A relevance score (0-10)\n",
    "        2. Key entities found (people, projects, products, terms, dates)\n",
    "\n",
    "        Return only a JSON object with NO direct quotes:\n",
    "        {{\n",
    "            \"{doc_id}\": {{\n",
    "                \"score\": 7,\n",
    "                \"entities\": {{\n",
    "                    \"people\": [\"name1\", \"name2\"],\n",
    "                    \"projects\": [\"project1\"],\n",
    "                    \"products\": [\"product1\"],\n",
    "                    \"terms\": [\"term1\"],\n",
    "                    \"dates\": [\"date1\"]\n",
    "                }}\n",
    "            }}\n",
    "        }}\n",
    "\n",
    "        Document to analyze:\n",
    "        Document ID: {doc_id}\n",
    "        Title: {title}\n",
    "        Content summary: {ocr_text}\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.model.generate_content(prompt)\n",
    "            response_text = response.text.strip()\n",
    "            \n",
    "            json_match = re.search(r'\\{.*\\}', response_text, re.DOTALL)\n",
    "            if json_match:\n",
    "                analysis = json.loads(json_match.group())\n",
    "                print(f\"Document {doc_id}: Score {analysis[doc_id]['score']}/10\")\n",
    "                return analysis\n",
    "                \n",
    "        except ValueError as e:\n",
    "            if \"reciting from copyrighted material\" in str(e):\n",
    "                print(f\"⚠️ Copyright detection for {doc_id} - using metadata only\")\n",
    "                return self._create_basic_analysis([doc])\n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing {doc_id}: {e}\")\n",
    "        \n",
    "        return self._create_basic_analysis([doc])\n",
    "\n",
    "    def _create_basic_analysis(self, docs: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        \"\"\"Create basic document analysis from metadata\"\"\"\n",
    "        analysis = {}\n",
    "        for doc in docs:\n",
    "            doc_id = doc['id']\n",
    "            \n",
    "            # Handle 'au' as either a list or string\n",
    "            authors = []\n",
    "            if 'au' in doc:\n",
    "                if isinstance(doc['au'], list):\n",
    "                    authors = doc['au']\n",
    "                elif isinstance(doc['au'], str):\n",
    "                    authors = [a.strip() for a in doc['au'].split(';') if a.strip()]\n",
    "            \n",
    "            date = doc.get('dd', '')\n",
    "            title = doc.get('ti', '')\n",
    "            title_terms = [t.lower() for t in title.split() if len(t) > 3] if isinstance(title, str) else []\n",
    "            \n",
    "            analysis[doc_id] = {\n",
    "                \"score\": 5,  # Neutral score\n",
    "                \"entities\": {\n",
    "                    \"people\": authors,\n",
    "                    \"projects\": [],\n",
    "                    \"products\": [],\n",
    "                    \"terms\": title_terms,\n",
    "                    \"dates\": [date] if date else []\n",
    "                }\n",
    "            }\n",
    "        return analysis\n",
    "\n",
    "    def find_surrounding_docs(self, doc_id: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any], List[Dict[str, Any]]]:\n",
    "        \"\"\"Find documents before and after the target document\"\"\"\n",
    "        target_response = requests.get(self.base_url, params={\n",
    "            'q': f'id:{doc_id}',\n",
    "            'wt': 'json',\n",
    "            'fl': 'id,au,ti,bn,dd,dt,availability,pg,attach,access,artifact'\n",
    "        }, verify=False)\n",
    "        \n",
    "        if target_response.status_code != 200:\n",
    "            return [], None, []\n",
    "            \n",
    "        response_docs = target_response.json()['response']['docs']\n",
    "        if not response_docs:\n",
    "            print(f\"Document {doc_id} not found\")\n",
    "            return [], None, []\n",
    "            \n",
    "        target_doc = response_docs[0]\n",
    "        if not self.is_public_doc(target_doc):\n",
    "            return [], None, []\n",
    "            \n",
    "        bates_num = target_doc.get('bn')\n",
    "        if not bates_num:\n",
    "            print(f\"No Bates number for document {doc_id}\")\n",
    "            return [], None, []\n",
    "        \n",
    "        before_params = {\n",
    "            'fq': [\n",
    "                '(collectioncode:\"pm\" OR collectioncode:\"msa\")',\n",
    "                'published:true',\n",
    "                f'bn:[* TO \"{bates_num}\"}}'\n",
    "            ],\n",
    "            'fl': 'id,au,ti,bn,dd,dt,availability,pg,attach,access,artifact',\n",
    "            'wt': 'json',\n",
    "            'rows': '3',\n",
    "            'sort': 'bn_sort desc'\n",
    "        }\n",
    "        \n",
    "        after_params = {\n",
    "            'fq': [\n",
    "                '(collectioncode:\"pm\" OR collectioncode:\"msa\")',\n",
    "                'published:true',\n",
    "                f'bn:{{\"{bates_num}\" TO *]'\n",
    "            ],\n",
    "            'fl': 'id,au,ti,bn,dd,dt,availability,pg,attach,access,artifact',\n",
    "            'wt': 'json',\n",
    "            'rows': '3',\n",
    "            'sort': 'bn_sort asc'\n",
    "        }\n",
    "        \n",
    "        before_url = self.build_url(before_params)\n",
    "        after_url = self.build_url(after_params)\n",
    "        \n",
    "        try:\n",
    "            before_docs = requests.get(before_url, verify=False, timeout=10)\n",
    "            after_docs = requests.get(after_url, verify=False, timeout=10)\n",
    "            \n",
    "            before_results = []\n",
    "            after_results = []\n",
    "            \n",
    "            if before_docs.status_code == 200:\n",
    "                before_results = [doc for doc in before_docs.json()['response']['docs'] \n",
    "                                if self.is_public_doc(doc)]\n",
    "            \n",
    "            if after_docs.status_code == 200:\n",
    "                after_results = [doc for doc in after_docs.json()['response']['docs'] \n",
    "                               if self.is_public_doc(doc)]\n",
    "\n",
    "            return before_results, target_doc, after_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error finding surrounding documents for {doc_id}: {e}\")\n",
    "            return [], None, []\n",
    "\n",
    "    def _get_sorted_documents(self) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Get documents sorted by weighted score\"\"\"\n",
    "        weighted_scores = []\n",
    "        for doc_id, doc_data in self.document_store.items():\n",
    "            if doc_data['analysis']:  # Only include analyzed documents\n",
    "                base_score = doc_data['analysis']['score']\n",
    "                frequency_bonus = min(self.document_frequencies[doc_id] * 0.5, 2.0)\n",
    "                weighted_scores.append((doc_id, base_score + frequency_bonus))\n",
    "        \n",
    "        return sorted(weighted_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    def _prepare_final_results(self) -> Dict[str, Any]:\n",
    "        \"\"\"Prepare final results with all relevant information\"\"\"\n",
    "        sorted_docs = self._get_sorted_documents()\n",
    "        final_results = {}\n",
    "        \n",
    "        for doc_id, weighted_score in sorted_docs:\n",
    "            doc_data = self.document_store[doc_id]\n",
    "            if doc_data['analysis']:  # Only include analyzed documents\n",
    "                final_results[doc_id] = {\n",
    "                    **doc_data['analysis'],\n",
    "                    'frequency': self.document_frequencies[doc_id],\n",
    "                    'weighted_score': weighted_score\n",
    "                }\n",
    "        \n",
    "        return final_results\n",
    "\n",
    "    def analyze_with_gemini(self, docs: List[Dict[Any, Any]], user_query: str, custom_prompt: str = None) -> str:\n",
    "        \"\"\"Generate final analysis using Gemini with simpler, more direct approach\"\"\"\n",
    "        try:\n",
    "            # Create a single temporary directory for full document texts\n",
    "            with tempfile.TemporaryDirectory() as temp_dir:\n",
    "                # Store all document texts in single file with clear separators\n",
    "                analysis_file = os.path.join(temp_dir, \"documents.txt\")\n",
    "                with open(analysis_file, 'w', encoding='utf-8') as f:\n",
    "                    for doc in docs:\n",
    "                        doc_id = doc.get('id')\n",
    "                        title = doc.get('ti', 'No title')\n",
    "                        date = doc.get('dd', 'No date')\n",
    "                        authors = ', '.join(doc.get('au', [])) if isinstance(doc.get('au'), list) else doc.get('au', 'Unknown')\n",
    "                        doc_type = doc.get('dt', 'Unknown type')\n",
    "                        ocr_text = self.document_store[doc_id]['ocr_text'] if doc_id in self.document_store else self.get_ocr_text(doc_id)\n",
    "                        \n",
    "                        f.write(f\"\"\"\n",
    "    === DOCUMENT START ===\n",
    "    ID: {doc_id}\n",
    "    Title: {title}\n",
    "    Date: {date}\n",
    "    Authors: {authors}\n",
    "    Type: {doc_type}\n",
    "\n",
    "    Content:\n",
    "    {ocr_text}\n",
    "\n",
    "    === DOCUMENT END ===\n",
    "\n",
    "    \"\"\")\n",
    "                \n",
    "                # Read all content\n",
    "                with open(analysis_file, 'r', encoding='utf-8') as f:\n",
    "                    all_docs_text = f.read()\n",
    "\n",
    "                # Create single comprehensive prompt\n",
    "                prompt = f\"\"\"Research Question: {user_query}\n",
    "\n",
    "    Analyze these tobacco industry documents. Create a detailed summary for each document in this exact format:\n",
    "\n",
    "    [document_id]: Summary should include:\n",
    "    - Document type and year\n",
    "    - Key findings or main points\n",
    "    - Notable people, organizations, or projects\n",
    "    - Specific data, numbers, or research findings\n",
    "    - Marketing strategies or business decisions\n",
    "    - Public health implications\n",
    "\n",
    "    Example of desired summary format:\n",
    "    [ysvj0228]: This 2008 American Journal of Public Health article examines how tobacco manufacturers manipulated menthol levels in cigarettes to target adolescents and young adults. The authors analyzed internal tobacco industry documents, conducted lab tests on various menthol cigarette brands, and reviewed data from the National Survey on Drug Use and Health. Their findings indicate that lower menthol levels, particularly in brands like Newport and Marlboro Milds, were more appealing to younger smokers because they masked the harshness of cigarettes, facilitating smoking initiation and nicotine addiction. Higher menthol levels were targeted toward long-term smokers. The study also reveals a significant increase in magazine advertising expenditures for menthol brands, despite a decline in overall cigarette sales.\n",
    "\n",
    "    Documents to analyze:\n",
    "\n",
    "    {all_docs_text}\n",
    "\n",
    "    Return ONLY document summaries in the specified format, with one blank line between each summary. Group related documents together if they share common themes or topics.\"\"\"\n",
    "\n",
    "                try:\n",
    "                    response = self.model.generate_content(\n",
    "                        prompt,\n",
    "                        generation_config=genai.GenerationConfig(\n",
    "                            candidate_count=1,\n",
    "                            max_output_tokens=8192,\n",
    "                            temperature=0.2,\n",
    "                            top_k=20,\n",
    "                            top_p=0.8\n",
    "                        )\n",
    "                    )\n",
    "                    return response.text.strip()\n",
    "                    \n",
    "                except ValueError as e:\n",
    "                    if \"reciting from copyrighted material\" in str(e):\n",
    "                        # Return metadata-only summaries\n",
    "                        return \"\\n\\n\".join([\n",
    "                            f\"[{doc.get('id', 'Unknown ID')}]: [Content restricted - Metadata only] \"\n",
    "                            f\"This {doc.get('dd', 'undated')} document titled '{doc.get('ti', 'No title')}' \"\n",
    "                            f\"is a {doc.get('dt', 'document')} authored by \"\n",
    "                            f\"{', '.join(doc.get('au', ['Unknown'])) if isinstance(doc.get('au'), list) else doc.get('au', 'Unknown')}.\"\n",
    "                            for doc in docs\n",
    "                        ])\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in document analysis: {e}\")\n",
    "                    return self._create_basic_metadata_summaries(docs)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error in overall analysis: {e}\")\n",
    "            return self._create_basic_metadata_summaries(docs)\n",
    "\n",
    "    def _create_basic_metadata_summaries(self, docs: List[Dict[str, Any]]) -> str:\n",
    "        \"\"\"Create basic summaries from metadata when analysis fails\"\"\"\n",
    "        return \"\\n\\n\".join([\n",
    "            f\"[{doc.get('id', 'Unknown ID')}]: Basic metadata - \"\n",
    "            f\"This {doc.get('dd', 'undated')} document titled '{doc.get('ti', 'No title')}' \"\n",
    "            f\"is a {doc.get('dt', 'document')} authored by \"\n",
    "            f\"{', '.join(doc.get('au', ['Unknown'])) if isinstance(doc.get('au'), list) else doc.get('au', 'Unknown')}.\"\n",
    "            for doc in docs\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = TobaccoDocAnalyzer()\n",
    "query = input(\"Enter your research question about tobacco documents: \")\n",
    "results = analyzer.analyze_topic(query)\n",
    "\n",
    "print(\"\\nFinal Analysis:\")\n",
    "print(results['final_analysis'])\n",
    "print(f\"\\nTotal documents analyzed: {results['document_count']}\")\n",
    "print(f\"Total documents seen: {results['total_documents_seen']}\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19638 total documents\n",
      "\n",
      "Document:\n",
      "Title: No title\n",
      "Date: 1999 August 18\n",
      "Type: ['memo']\n",
      "Collection: ['Philip Morris Records', 'Master Settlement Agreement']\n",
      "\n",
      "Document:\n",
      "Title: SPECIAL REPORT TO THE FTC\n",
      "Date: 1993 July 06\n",
      "Type: ['memo']\n",
      "Collection: ['Philip Morris Records', 'Master Settlement Agreement']\n",
      "\n",
      "Document:\n",
      "Title: FTC SPECIAL REPORT FOR 19960000\n",
      "Date: 1997 April 22\n",
      "Type: ['memo']\n",
      "Collection: ['Philip Morris Records', 'Master Settlement Agreement']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "base_url = \"https://solr.idl.ucsf.edu/solr/ltdl3/select\"\n",
    "\n",
    "params = {\n",
    "    'q': 'menthol',  \n",
    "    'fq': [  \n",
    "        'availability:public',\n",
    "        'dd:[19900101 TO 19991231]',\n",
    "        'dt:memo',\n",
    "        'collection:\"Philip Morris\"'\n",
    "    ],\n",
    "    'wt': 'json',\n",
    "    'rows': 10     \n",
    "}\n",
    "\n",
    "response = requests.get(base_url, params=params, verify=False)\n",
    "results = response.json()\n",
    "print(f\"Found {results['response']['numFound']} total documents\")\n",
    "\n",
    "# Look at documents including their type\n",
    "if results['response']['numFound'] > 0:\n",
    "    for doc in results['response']['docs'][:3]:\n",
    "        print(\"\\nDocument:\")\n",
    "        print(f\"Title: {doc.get('ti', 'No title')}\")\n",
    "        print(f\"Date: {doc.get('dd', 'No date')}\")\n",
    "        print(f\"Type: {doc.get('dt', 'No type')}\")\n",
    "        print(f\"Collection: {doc.get('collection', 'No collection')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 documents with empty titles\n",
      "\n",
      "ID: qpbh0182\n",
      "Date: 1997 October 29\n",
      "Type: ['deposition', 'trial transcript']\n",
      "Authors: ['BLANK,MS', 'BRECKENRIDGE,JAE', 'GADDES,A', 'MAISTROS,JD', 'NUNLEY,LD III', 'SHUB,J', 'SWAIN,JW', 'WEBB,K']\n",
      "Bates: 2062814125-2062814326\n",
      "--------------------------------------------------\n",
      "ID: frdx0055\n",
      "Date: 1997 October 09\n",
      "Type: ['deposition', 'trial transcript']\n",
      "Authors: ['SUPREME COURT OF THE STATE OF NEW YORK COUNTY OF NEW YORK']\n",
      "Bates: 94669244-94669494B\n",
      "--------------------------------------------------\n",
      "ID: plcm0166\n",
      "Date: 1997 October 24\n",
      "Type: ['pleading']\n",
      "Authors: No authors\n",
      "Bates: 2062817661-2062817669\n",
      "--------------------------------------------------\n",
      "ID: tmmh0055\n",
      "Date: 1997 October 23\n",
      "Type: ['deposition', 'trial transcript']\n",
      "Authors: ['COUNTY OF NEW YORK', 'SUPREME COURT OF THE STATE OF NEW YORK']\n",
      "Bates: 94669755-94670002\n",
      "--------------------------------------------------\n",
      "ID: ftwx0228\n",
      "Date: 2004 October 07\n",
      "Type: ['abstract']\n",
      "Authors: ['AEGEAN AGRICULTURAL RESEARCH INS', 'CENTRAL LAB OF GENERAL ECOLOGY', 'CENTRAL RESEARCH STATION FOR TOBACCO GROWING + INDUSTRIALIZATION', 'COLLEGE OF NYIREGYHAZA', 'COMPLEX EXPERIMENTAL COMPLEX', 'DIMON', 'FACULTY OF AGRICULTURE', 'FACULTY OF AGRICULTURE SCIENCE', 'GERMANS TECHNOLOGY GROUP', 'GUILAN TOBACCO RESEARCH CENTER', 'HUNGARIAN INST OF AGRICULTURAL ENGINEERING', 'INST FOR PLANT PROTECTION IN AGRICULTURE + FORESTRY OF REPUBLIC OF CROATIA', 'INST SERBIA CENTER ATR', 'ISTA', 'ISTITUTO SPERIMENTALE PER IL TABACCO', 'LABORATOIRE DE BIOTECHNOLOGIE ET AMELIORATION DES PLANTES', 'OTRC', 'RESEARCH INST FOR TOBACCO', 'ROMANIAN PARLIAMENT', 'TI, TOBACCO INST', 'TIRTASH TOBACCO RESEARCH INST', 'TOBACCO RESEARCH BOARD', 'TOBACCO RESEARCH STATION', 'TOBACCO SLATINA', 'UNIDO', 'ABET,M', 'AHIFAR,H', 'AKGUNLU,A', 'ALAVI,R', 'ASCIONE,S', 'ASSIMI,MH', 'BIGLOUIE,MH', 'BOLD,I', 'BRUTARU,L', 'CARNICI,M', 'CERSOSIMO,A', 'CHASEKWA,B', 'CHOOBEH,R', 'CIUPERCA,A', 'CONTILLO,R', 'DARVISHZADEH,R', 'DUMITRASCU,P', 'EMIL,N', 'ESFAHANY,M', 'FENYVESI,L', 'FESTA,FP', 'FILIPOSKI,K', 'GHOLIZADEH,A', 'HAMEL,D', 'HODGATI,A', 'HRISTEVA,T', 'IANTCHEVA,M', 'INTERLANDI,G', 'IORDANOV,VI', 'KEREKES,B', 'LOMBARDI,DA', 'MARKOVIC,D', 'MAZARURA,U', 'MESBAH,M', 'MESIC,H', 'MILITARU,DC', 'MILOSAVLJEVIC,S', 'MOEZI,AA', 'NABIPOUR,A', 'NAMVARE,RE', 'NAPOLITANO,A', 'NAUMOVA,S', 'NZUMA,JK', 'PANCIU,A', 'PAUNESCU,AD', 'PAUNESCU,M', 'PELIVANOSKA,V', 'PETROVIC,B', 'RADULOVIC,V', 'RANJBAR,CM', 'SABETI,MA', 'SABOVLJEVIC,R', 'SANZ,R', 'SCHREURS,F', 'SHAMEL,RMT', 'SHOAI,DM', 'SIAVASH,MS', 'STANKOVIC,Z', 'TAGHAVI,R', 'TRAJKOSKI,J', 'TURSIC,I', 'VESSELINA,M', 'YAZAN,G']\n",
      "Bates: 06340838-06340853\n",
      "--------------------------------------------------\n",
      "ID: mkpx0074\n",
      "Date: 1997\n",
      "Type: ['regulation']\n",
      "Authors: ['MS LEGISLATURE', 'MCBRIDE']\n",
      "Bates: 2081876197-2081876213\n",
      "--------------------------------------------------\n",
      "ID: nqmh0045\n",
      "Date: No date\n",
      "Type: No type\n",
      "Authors: ['The Tobacco Institute']\n",
      "Bates: 2024939995-2024940026\n",
      "--------------------------------------------------\n",
      "ID: xsyl0228\n",
      "Date: 2004 October 07\n",
      "Type: ['abstract']\n",
      "Authors: ['AGRICULTURAL RESEARCH + EXTENSION TRUST', 'CANADIAN TOBACCO RESEARCH FOUNDATION', 'CENTRAL TOBACCO RESEARCH INST', 'HENAN PROVINCIAL TOBACCO', 'JAPAN TOBACCO', 'KAGOSHIMA LEAF TOBACCO TECHNOLOGY CENTER', 'LAB OF AGRONOMY', 'LAB OF PORNOLOGY', 'LABORATOIRE DE BIOTECHNOLOGIE ET AMELIORATION DES PLANTES', 'LEAF TOBACCO RESEARCH CENTER', 'LOWVELD TOBACCO GROWERS ASSN', 'LTGA', 'NC STATE UNIV', 'ORUMIEH TOBACCO RESEARCH CENTRE', 'TARBLAT MODARRES UNIV', 'TOBACCO INST OF GREECE', 'TOBACCO RESEARCH BOARD', 'AMANKWA,GA', 'BRAMMALL,RA', 'CHASEKWA,B', 'DARVISHZADEH,R', 'DEHGHANI,H', 'DEO,SK', 'DEVOS,M', 'DIVANIDIS,S', 'DOI,I', 'EMAMI,F', 'FAN,Y', 'FISHER,LR', 'FONTAINE,B', 'GOMONDA,RWJ', 'GORANOVIC,S', 'HAJI,HH', 'KARAIVAZOGLOU,NA', 'KOSAKA,Y', 'KRISHNAMURTHY,V', 'MAEKAWA,Y', 'MARKOVIC,D', 'MAZARURA,U', 'MISHRA,S', 'NAKAKAWAJI,T', 'NAMVARE,RE', 'NTHONYIWA,AS', 'NZUMA,JK', 'ONO,T', 'OSAMURA,K', 'PAPAKOSTA,DK', 'PETROVIC,B', 'RAVISANKAR,H', 'SABOVLJEVIC,R', 'SALEHI,B', 'SCHOLTZ,A', 'SFACIOTAKIS,EM', 'SIHANDE,LAGO', 'SMITH,WD', 'STANKOVIC,Z', 'TAKAHASHI,A', 'ZHANG,X']\n",
      "Bates: 06340810-06340819\n",
      "--------------------------------------------------\n",
      "ID: qpfg0072\n",
      "Date: 1997 October 01\n",
      "Type: ['regulation']\n",
      "Authors: ['MORRONI']\n",
      "Bates: 82250945-82250954\n",
      "--------------------------------------------------\n",
      "ID: hfgn0012\n",
      "Date: 1998 February 02\n",
      "Type: ['pleading']\n",
      "Authors: ['AMER, AMERICAN TOBACCO', 'BW, BROWN & WILLIAMSON', 'CTR, COUNCIL FOR TOBACCO RESEARCH', 'HILL KNOWLTON', 'LM, LIGGETT & MYERS', 'LOR, LORILLARD', 'MUNGER TOLLES', 'PHILIP MORRIS INCORPORATED', 'RJR, R.J.REYNOLDS', 'TI, TOBACCO INST', 'US TOBACCO', 'COLLINS,DP', 'OLSON,RL', 'STONE,GP', 'WEISBURD,SB']\n",
      "Bates: 2074692027-2074692033\n",
      "--------------------------------------------------\n",
      "ID: qnfh0095\n",
      "Date: 1999 February 18\n",
      "Type: ['pleading', 'publication']\n",
      "Authors: ['USDC ED NY', 'WEST']\n",
      "Bates: 98074081\n",
      "--------------------------------------------------\n",
      "ID: pjkh0011\n",
      "Date: 1998 October 15\n",
      "Type: ['email', 'report']\n",
      "Authors: No authors\n",
      "Bates: 531804981-531804989\n",
      "--------------------------------------------------\n",
      "ID: pkbj0011\n",
      "Date: 2002 April 04\n",
      "Type: ['email', 'report']\n",
      "Authors: No authors\n",
      "Bates: 531315878-531315886\n",
      "--------------------------------------------------\n",
      "ID: jsyl0228\n",
      "Date: 2004 January 01\n",
      "Type: ['abstract']\n",
      "Authors: ['AGRICULTURAL RESEARCH + EXTENSION TRUST', 'ALTADIS INSTITUT DU TABAC', 'CENTRAL TOBACCO RESEARCH INST', 'ISTITUTO SPERIMENTALE PER IL TABACCO', 'JAPAN TOBACCO', 'LORILLARD', 'NC UNIV RALEIGH', 'TOBACCO RESEARCH BOARD', 'UNIV OF KY', 'UNIV OF KY LEXINGTON', 'UNIV OF KY PRINCETON', 'YUNNAN TOBACCO SCIENCE INST YUXI', 'YUXI HONGTA TOBACCO GROUP', 'ZHENGZHOU TOBACCO RESEARCH INST OF CNTC', 'ABET,M', 'BAILEY,WA', 'BARBATO,L', 'BURTON,HR', 'BUSH,LP', 'CALVERT,J', 'CHANGAYA,BA', 'CHASEKWA,B', 'CHINKHUNTHIA,JK', 'COZZOLINO,E', 'CUCINIELLO,A', 'DELPIANO,L', 'DUNCAN,GA', 'ELLIOT,PE', 'FANNIN,FF', 'FUJII,S', 'GIRARD,C', 'GONDWE,WK', 'HARADA,K', 'HIROSE,E', 'ISHIWATA,Y', 'JACK,AM', 'JACQUET,L', 'KOGA,K', 'LEVIN,JS', 'LEWIN,RS', 'LEWIS,RS', 'LI,TF', 'LI,X', 'MAZARURA,U', 'MIDDLETON,DC', 'MILLA,SR', 'MORI,K', 'NAGESWARARAO,S', 'NARIMATSU,C', 'NOGUCHI,S', 'OZAWA,H', 'POISSON,C', 'RITCHEY,E', 'ROTON,C', 'SAITO,H', 'SHEW,HD', 'SHI,H', 'SINGH,DK', 'SITARAMAIAH,S', 'SMITH,D', 'SORRENTINO,C', 'SREEDHAR,U', 'TEMBANI,PM', 'VENKATESWARLU,P', 'WANG,A', 'ZHU,M']\n",
      "Bates: 06340826-06340837\n",
      "--------------------------------------------------\n",
      "ID: prmy0062\n",
      "Date: 1996\n",
      "Type: ['computer printout', 'regulation']\n",
      "Authors: ['WEST']\n",
      "Bates: 2072325333-2072325340\n",
      "--------------------------------------------------\n",
      "ID: zkwp0069\n",
      "Date: 1997 April 10\n",
      "Type: ['pleading']\n",
      "Authors: ['USDC MIDDLE DISTRICT FL TAMPA DIVISION', 'KOVACHEVICH,EA']\n",
      "Bates: 95519658-95519669\n",
      "--------------------------------------------------\n",
      "ID: ktwx0228\n",
      "Date: 2004 October 07\n",
      "Type: ['abstract']\n",
      "Authors: ['BAYER CROPSCIENCE MILAN', 'CENTRAL RESEARCH STATION FOR TOBACCO GROWING + INDUSTRIALIZATION', 'CENTRAL TOBACCO RESEARCH INST', 'COMPLEX EXPERIMENTAL STATION', 'FUJI FLAVOR', 'GUILAN UNIV RASHT', 'INST OF GENETICS', 'INST OF SOIL SCIENCE + PLANT CULTIVATION', 'ISTITUTO SPERIMENTALE PER IL TABACCO', 'KT+G CENTRAL RESEARCH INST', 'ROMANIAN PARLIAMENT', 'SPECHT + PARTNER', 'TI, TOBACCO INST', 'TIRTASH TOBACCO RESEARCH INST BEHSHAHR', 'TOBACCO RESEARCH CENTER', 'ANSPACH,T', 'ARCANGELI,G', 'BERBEC,A', 'BIONDANI,MC', 'BURCEA,AM', 'CAIAZZO,R', 'CARELLA,A', 'CERSOSIMO,A', 'CHUMAN,T', 'CHUNG,Y', 'CONTIERO,M', 'DALILI,SR', 'DEO,SK', 'DEOSINGH,K', 'DIMA,A', 'HONARNEJAD,R', 'IANTCHEVA,AM', 'KANG,Y', 'KHATERI,H', 'KORUBINALEKSOSKA,A', 'LAHOZ,E', 'LASKOWSKA,D', 'LINKERHAGNER,M', 'MAHTABI,RA', 'MURTHY,TGK', 'NIKOVA,V', 'PANDEVA,R', 'PAUNESCO,M', 'PAUNESCU,AD', 'PETKOVA,A', 'PORRONE,F', 'RAVI,SH', 'RAVISANKAR,H', 'SANISLAV,N', 'SANNINO,L', 'SASAKI,R', 'SCHIEBEL,M', 'SHINODA,K', 'SHOAEI,DM', 'STANESCU,V', 'TATENO,A', 'TROJAKGOLUCH,A', 'YU,Y']\n",
      "Bates: 06340854-06340862\n",
      "--------------------------------------------------\n",
      "ID: zmcm0005\n",
      "Date: 1962 July 19\n",
      "Type: ['publication']\n",
      "Authors: ['REPUBLIC']\n",
      "Bates: 501862932\n",
      "--------------------------------------------------\n",
      "ID: lyll0006\n",
      "Date: 2005 February 28\n",
      "Type: ['email', 'letter']\n",
      "Authors: ['BLIXT CA']\n",
      "Bates: 551410286-551410294\n",
      "--------------------------------------------------\n",
      "ID: jjfc0086\n",
      "Date: 1962 April 17\n",
      "Type: ['publication']\n",
      "Authors: ['PIONEER PRESS']\n",
      "Bates: 501863098\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Initialize analyzer\n",
    "analyzer = TobaccoDocAnalyzer()\n",
    "\n",
    "# Build search parameters for empty titles\n",
    "params = {\n",
    "    'q': 'ti:\"tobacco',  # Search for empty title field\n",
    "    'fq': ['availability:public'],  # Only public documents\n",
    "    'wt': 'json',\n",
    "    'rows': '20',  # Get 20 results\n",
    "    'fl': 'id,au,ti,bn,dd,dt'  # Fields to return\n",
    "}\n",
    "\n",
    "# Execute search\n",
    "response = requests.get(analyzer.base_url, params=params, verify=False)\n",
    "docs = response.json()['response']['docs']\n",
    "\n",
    "# Display results\n",
    "print(f\"Found {len(docs)} documents with empty titles\\n\")\n",
    "for doc in docs:\n",
    "    print(f\"ID: {doc['id']}\")\n",
    "    print(f\"Date: {doc.get('dd', 'No date')}\")\n",
    "    print(f\"Type: {doc.get('dt', 'No type')}\")\n",
    "    print(f\"Authors: {doc.get('au', 'No authors')}\")\n",
    "    print(f\"Bates: {doc.get('bn', 'No bates number')}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "38cca0c38332a56087b24af0bc80247f4fced29cb4f7f437d91dc159adec9c4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
