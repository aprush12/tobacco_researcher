{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from typing import List, Dict, Any, Set, Tuple\n",
    "from collections import defaultdict\n",
    "import google.generativeai as genai\n",
    "import urllib3\n",
    "from urllib.parse import urlencode\n",
    "import time\n",
    "from difflib import SequenceMatcher\n",
    "import tempfile\n",
    "import os\n",
    "import re\n",
    "\n",
    "class TobaccoDocAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://solr.idl.ucsf.edu/solr/ltdl3/select\"\n",
    "        self.ocr_base = \"https://download.industrydocuments.ucsf.edu/\"\n",
    "        genai.configure(api_key='AIzaSyDl7OsvN8gB8v33BkcIzmSuflwia7YOrQk')\n",
    "        self.model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "        # Document tracking\n",
    "        self.document_frequencies = defaultdict(int)\n",
    "        self.document_store = {}  # Single source of truth for all document data\n",
    "        self.title_hash = set()  # Store normalized titles for fast duplicate checking\n",
    "\n",
    "    def _normalize_title(self, title: str) -> str:\n",
    "        \"\"\"Create a normalized version of the title for comparison\"\"\"\n",
    "        if not title:\n",
    "            return \"\"\n",
    "        # Remove punctuation, convert to lowercase, and remove extra whitespace\n",
    "        normalized = ''.join(c.lower() for c in title if c.isalnum() or c.isspace())\n",
    "        return ' '.join(normalized.split())\n",
    "\n",
    "    \n",
    "    def _add_to_document_store(self, doc: Dict[str, Any]) -> bool:\n",
    "        \"\"\"Add document to store if it's not a duplicate. Return True if added.\"\"\"\n",
    "        doc_id = doc['id']\n",
    "        title = doc.get('ti', '')\n",
    "        \n",
    "        # Skip if exact ID already exists\n",
    "        if doc_id in self.document_store:\n",
    "            return False\n",
    "            \n",
    "        # Get normalized title\n",
    "        norm_title = self._normalize_title(title)\n",
    "        if not norm_title:  # If no title, just add the doc\n",
    "            self.document_store[doc_id] = {\n",
    "                'metadata': doc,\n",
    "                'analysis': None,\n",
    "                'ocr_text': None\n",
    "            }\n",
    "            return True\n",
    "            \n",
    "        # Check for duplicate title\n",
    "        if norm_title in self.title_hash:\n",
    "            print(f\"Skipping document {doc_id} due to seen title\")\n",
    "            return False\n",
    "            \n",
    "        # Add document and its normalized title\n",
    "        self.title_hash.add(norm_title)\n",
    "        self.document_store[doc_id] = {\n",
    "            'metadata': doc,\n",
    "            'analysis': None,\n",
    "            'ocr_text': None\n",
    "        }\n",
    "        return True\n",
    "    def build_url(self, params):\n",
    "        \"\"\"Build URL manually to match exact format from website\"\"\"\n",
    "        base = self.base_url + \"?q=*:*\"\n",
    "        for fq in params.get('fq', []):\n",
    "            base += f\"&fq={requests.utils.quote(fq)}\"\n",
    "        for key, value in params.items():\n",
    "            if key != 'fq':\n",
    "                base += f\"&{key}={requests.utils.quote(str(value))}\"\n",
    "        return base\n",
    "\n",
    "    def is_public_doc(self, doc):\n",
    "        \"\"\"Check if document is public and unrestricted\"\"\"\n",
    "        availability = doc.get('availability', [])\n",
    "        return \"public\" in availability or \"no restrictions\" in availability\n",
    "\n",
    "    def get_ocr_text(self, doc_id: str) -> str:\n",
    "        \"\"\"Gets OCR text for a document\"\"\"\n",
    "        path_segment = '/'.join(list(doc_id[:4].lower()))\n",
    "        url = f\"{self.ocr_base}{path_segment}/{doc_id.lower()}/{doc_id.lower()}.ocr\"\n",
    "        try:\n",
    "            response = requests.get(url, verify=False, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                return response.text\n",
    "            return \"\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting OCR text for {doc_id}: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def generate_search_strategies(self, user_query: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Generate multiple search strategies based on the user query\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        Given this research question about tobacco documents: \"{user_query}\"\n",
    "        Generate 3 different search strategies. Each strategy should contain 2-4 key terms that would help find relevant documents.\n",
    "        Return your response in this exact JSON format with no additional text:\n",
    "        {{\n",
    "            \"strategies\": [\n",
    "                {{\n",
    "                    \"search_terms\": \"term1 term2\",\n",
    "                    \"filters\": {{}},\n",
    "                    \"rationale\": \"why this might work\"\n",
    "                }}\n",
    "            ]\n",
    "        }}\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.model.generate_content(prompt)\n",
    "            response_text = response.text.strip()\n",
    "            \n",
    "            # Try to find JSON in the response\n",
    "            import re\n",
    "            json_match = re.search(r'\\{.*\\}', response_text, re.DOTALL)\n",
    "            if json_match:\n",
    "                strategies = json.loads(json_match.group())\n",
    "                print(\"\\nGenerated search strategies:\")\n",
    "                for idx, strat in enumerate(strategies['strategies'], 1):\n",
    "                    print(f\"\\nStrategy {idx}:\")\n",
    "                    print(f\"Terms: {strat['search_terms']}\")\n",
    "                    print(f\"Filters: {strat['filters']}\")\n",
    "                return strategies['strategies']\n",
    "            else:\n",
    "                return self._fallback_search_strategy(user_query)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating search strategies: {e}\")\n",
    "            return self._fallback_search_strategy(user_query)\n",
    "\n",
    "    def _fallback_search_strategy(self, query: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Generate basic search strategies from query terms\"\"\"\n",
    "        print(\"\\nFalling back to query text parsing\")\n",
    "        query_terms = query.lower()\n",
    "        # Extract any quoted phrases or word combinations\n",
    "        terms = re.findall(r'\"([^\"]*)\"|\\b\\w+\\s+\\w+\\b|\\b\\w+\\b', query_terms)\n",
    "        \n",
    "        strategies = []\n",
    "        if len(terms) >= 2:\n",
    "            strategies.append({\n",
    "                \"search_terms\": f\"{terms[0]} AND {terms[1]}\",\n",
    "                \"filters\": {},\n",
    "                \"rationale\": \"Primary terms combination\"\n",
    "            })\n",
    "        \n",
    "        if len(terms) >= 3:\n",
    "            strategies.append({\n",
    "                \"search_terms\": f\"{terms[1]} AND {terms[2]}\",\n",
    "                \"filters\": {},\n",
    "                \"rationale\": \"Secondary terms combination\"\n",
    "            })\n",
    "        \n",
    "        all_terms = \" AND \".join(terms[:4])\n",
    "        strategies.append({\n",
    "            \"search_terms\": all_terms,\n",
    "            \"filters\": {},\n",
    "            \"rationale\": \"Combined key terms\"\n",
    "        })\n",
    "        \n",
    "        return strategies\n",
    "\n",
    "    def execute_search(self, strategy: Dict[str, Any], max_results: int = 50) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Execute a single search strategy\"\"\"\n",
    "        params = {\n",
    "            'q': strategy['search_terms'],\n",
    "            'fq': ['availability:public'],\n",
    "            'wt': 'json',\n",
    "            'rows': str(max_results),\n",
    "            'sort': 'score desc',\n",
    "            'fl': 'id,au,ti,bn,dd,dt,availability,pg,attach,access,artifact'\n",
    "        }\n",
    "        \n",
    "        # Add strategy filters\n",
    "        for field, value in strategy['filters'].items():\n",
    "            params['fq'].append(f'{field}:{value}')\n",
    "\n",
    "        try:\n",
    "            response = requests.get(self.base_url, params=params, verify=False)\n",
    "            if response.status_code == 200:\n",
    "                return [doc for doc in response.json()['response']['docs'] \n",
    "                       if self.is_public_doc(doc)]\n",
    "        except Exception as e:\n",
    "            print(f\"Error executing search: {e}\")\n",
    "        return []\n",
    "\n",
    "    def analyze_topic(self, user_query: str, max_iterations: int = 3) -> Dict[str, Any]:\n",
    "        \"\"\"Main analysis pipeline\"\"\"\n",
    "        print(f\"\\nStarting analysis for: {user_query}\")\n",
    "        \n",
    "        # Reset tracking for new analysis\n",
    "        self.document_frequencies.clear()\n",
    "        self.document_store.clear()\n",
    "        self.title_hash.clear()  # Clear title hash\n",
    "        \n",
    "        # Phase 1: Initial Search\n",
    "        strategies = self.generate_search_strategies(user_query)\n",
    "        \n",
    "        for strategy in strategies:\n",
    "            print(f\"\\nExecuting strategy: {strategy['search_terms']}\")\n",
    "            docs = self.execute_search(strategy)\n",
    "            if docs:\n",
    "                new_docs = []\n",
    "                for doc in docs:\n",
    "                    doc_id = doc['id']\n",
    "                    self.document_frequencies[doc_id] += 1\n",
    "                    if self._add_to_document_store(doc):  # Use new method\n",
    "                        new_docs.append(doc)\n",
    "                \n",
    "                # Process only new documents in batches\n",
    "                if new_docs:\n",
    "                    for i in range(0, len(new_docs), 5):\n",
    "                        batch = new_docs[i:i+5]\n",
    "                        self._process_document_batch(batch, user_query)\n",
    "            time.sleep(2)  # Rate limiting\n",
    "        \n",
    "        # Phase 2: Expansion\n",
    "        iteration = 0\n",
    "        while iteration < max_iterations:\n",
    "            print(f\"\\nExpansion iteration {iteration + 1}\")\n",
    "            \n",
    "            # Sort documents by weighted score\n",
    "            sorted_docs = self._get_sorted_documents()[:15]\n",
    "            \n",
    "            new_docs_count = 0\n",
    "            for doc_id, _ in sorted_docs:\n",
    "                before_docs, target, after_docs = self.find_surrounding_docs(doc_id)\n",
    "                \n",
    "                # Process new documents found\n",
    "                new_docs = []\n",
    "                for doc in before_docs + after_docs:\n",
    "                    doc_id = doc['id']\n",
    "                    self.document_frequencies[doc_id] += 1\n",
    "                    if self._add_to_document_store(doc):  # Use new method\n",
    "                        new_docs.append(doc)\n",
    "                        new_docs_count += 1\n",
    "                \n",
    "                # Analyze new documents\n",
    "                if new_docs:\n",
    "                    self._process_document_batch(new_docs, user_query)\n",
    "                time.sleep(1)\n",
    "            \n",
    "            print(f\"Found {new_docs_count} new documents\")\n",
    "            print(f\"Total unique documents: {len(self.document_store)}\")\n",
    "            \n",
    "            # Check for saturation\n",
    "            if new_docs_count < 3:\n",
    "                print(\"Reached saturation\")\n",
    "                break\n",
    "            \n",
    "            iteration += 1\n",
    "        \n",
    "        # Prepare final results\n",
    "        final_results = self._prepare_final_results()\n",
    "        \n",
    "        # Get top docs for final analysis\n",
    "        top_docs = [self.document_store[doc_id]['metadata'] \n",
    "                for doc_id in list(final_results.keys())[:10]]\n",
    "        \n",
    "        final_analysis = self.analyze_with_gemini(top_docs, user_query)\n",
    "        \n",
    "        return {\n",
    "            'raw_results': final_results,\n",
    "            'final_analysis': final_analysis if top_docs else \"No documents found\",\n",
    "            'document_count': len(final_results),\n",
    "            'total_documents_seen': len(self.document_store),\n",
    "            'frequency_statistics': dict(self.document_frequencies)\n",
    "        }\n",
    "\n",
    "    def _process_document_batch(self, docs: List[Dict[str, Any]], query_context: str):\n",
    "        \"\"\"Process a batch of new documents\"\"\"\n",
    "        # Get OCR text for batch\n",
    "        for doc in docs:\n",
    "            doc_id = doc['id']\n",
    "            if not self.document_store[doc_id]['ocr_text']:\n",
    "                self.document_store[doc_id]['ocr_text'] = self.get_ocr_text(doc_id)\n",
    "        \n",
    "        # Analyze batch\n",
    "        analysis = self.analyze_document_batch(docs, query_context)\n",
    "        \n",
    "        # Store analysis results\n",
    "        for doc_id, doc_analysis in analysis.items():\n",
    "            if doc_id in self.document_store:\n",
    "                self.document_store[doc_id]['analysis'] = doc_analysis\n",
    "\n",
    "    def analyze_document_batch(self, docs: List[Dict[str, Any]], query_context: str) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze documents in batches with fallback to individual processing for problematic batches\"\"\"\n",
    "        if not docs:\n",
    "            return {}\n",
    "        \n",
    "        BATCH_SIZE = 5\n",
    "        batch_results = {}\n",
    "        \n",
    "        # Define the JSON example template separately\n",
    "        EXAMPLE_JSON = (\n",
    "            '{\\n'\n",
    "            '    \"doc_id1\": {\\n'\n",
    "            '        \"score\": 7,\\n'\n",
    "            '        \"entities\": {\\n'\n",
    "            '            \"people\": [\"name1\", \"name2\"],\\n'\n",
    "            '            \"projects\": [\"project1\"],\\n'\n",
    "            '            \"products\": [\"product1\"],\\n'\n",
    "            '            \"terms\": [\"term1\"],\\n'\n",
    "            '            \"dates\": [\"date1\"]\\n'\n",
    "            '        }\\n'\n",
    "            '    }\\n'\n",
    "            '}'\n",
    "        )\n",
    "        \n",
    "        # First try batch processing\n",
    "        for i in range(0, len(docs), BATCH_SIZE):\n",
    "            batch = docs[i:i+BATCH_SIZE]\n",
    "            doc_texts = []\n",
    "            batch_ids = []\n",
    "            \n",
    "            for doc in batch:\n",
    "                doc_id = doc['id']\n",
    "                title = doc.get('ti', 'No title')\n",
    "                batch_ids.append(doc_id)\n",
    "                doc_texts.append(\n",
    "                    f\"Document ID: {doc_id}\\n\"\n",
    "                    f\"Title: {title}\\n\"\n",
    "                    f\"Content:\\n{self.document_store[doc_id]['ocr_text'][:3000]}...\"\n",
    "                )\n",
    "\n",
    "            prompt = (\n",
    "                f\"Research context: {query_context}\\n\\n\"\n",
    "                \"Analyze these tobacco industry documents. For each document, provide:\\n\"\n",
    "                \"1. A relevance score (0-10)\\n\"\n",
    "                \"2. Key entities found (people, projects, products, terms, dates)\\n\\n\"\n",
    "                f\"Return only a JSON object like this example:\\n{EXAMPLE_JSON}\\n\\n\"\n",
    "                \"Documents to analyze:\\n\\n\"\n",
    "                f\"{chr(10) + '---' + chr(10)}\".join(doc_texts)\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                response = self.model.generate_content(prompt)\n",
    "                response_text = response.text.strip()\n",
    "                \n",
    "                # Try to find JSON in the response\n",
    "                json_match = re.search(r'\\{.*\\}', response_text, re.DOTALL)\n",
    "                if json_match:\n",
    "                    analysis = json.loads(json_match.group())\n",
    "                    print(f\"\\nScores for batch:\")\n",
    "                    for doc_id, details in analysis.items():\n",
    "                        print(f\"Document {doc_id}: Score {details['score']}/10\")\n",
    "                    batch_results.update(analysis)\n",
    "                    continue  # Move to next batch if successful\n",
    "                \n",
    "            except ValueError as e:\n",
    "                if \"reciting from copyrighted material\" in str(e):\n",
    "                    print(f\"\\n⚠️ Copyright detection in batch {i//BATCH_SIZE + 1} - processing documents individually\")\n",
    "                    # Process problematic batch one by one\n",
    "                    for doc in batch:\n",
    "                        doc_id = doc['id']\n",
    "                        try:\n",
    "                            individual_result = self._analyze_single_document(doc, query_context)\n",
    "                            batch_results.update(individual_result)\n",
    "                        except Exception as doc_error:\n",
    "                            print(f\"Error with document {doc_id}: {doc_error}\")\n",
    "                            batch_results.update(self._create_basic_analysis([doc]))\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\nError in batch {i//BATCH_SIZE + 1}: {e}\")\n",
    "                # Fallback to basic analysis for the entire batch\n",
    "                batch_results.update(self._create_basic_analysis(batch))\n",
    "            \n",
    "            time.sleep(0.5)  # Rate limiting between batches\n",
    "        \n",
    "        return batch_results\n",
    "\n",
    "    def _analyze_single_document(self, doc: Dict[str, Any], query_context: str) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze a single document when batch processing fails\"\"\"\n",
    "        doc_id = doc['id']\n",
    "        title = doc.get('ti', 'No title')\n",
    "        ocr_text = self.document_store[doc_id]['ocr_text'][:3000] if self.document_store[doc_id]['ocr_text'] else ''\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        Research context: {query_context}\n",
    "\n",
    "        Analyze this tobacco industry document. Instead of directly quoting, summarize:\n",
    "        1. A relevance score (0-10)\n",
    "        2. Key entities found (people, projects, products, terms, dates)\n",
    "\n",
    "        Return only a JSON object with NO direct quotes:\n",
    "        {{\n",
    "            \"{doc_id}\": {{\n",
    "                \"score\": 7,\n",
    "                \"entities\": {{\n",
    "                    \"people\": [\"name1\", \"name2\"],\n",
    "                    \"projects\": [\"project1\"],\n",
    "                    \"products\": [\"product1\"],\n",
    "                    \"terms\": [\"term1\"],\n",
    "                    \"dates\": [\"date1\"]\n",
    "                }}\n",
    "            }}\n",
    "        }}\n",
    "\n",
    "        Document to analyze:\n",
    "        Document ID: {doc_id}\n",
    "        Title: {title}\n",
    "        Content summary: {ocr_text}\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.model.generate_content(prompt)\n",
    "            response_text = response.text.strip()\n",
    "            \n",
    "            json_match = re.search(r'\\{.*\\}', response_text, re.DOTALL)\n",
    "            if json_match:\n",
    "                analysis = json.loads(json_match.group())\n",
    "                print(f\"Document {doc_id}: Score {analysis[doc_id]['score']}/10\")\n",
    "                return analysis\n",
    "                \n",
    "        except ValueError as e:\n",
    "            if \"reciting from copyrighted material\" in str(e):\n",
    "                print(f\"⚠️ Copyright detection for {doc_id} - using metadata only\")\n",
    "                return self._create_basic_analysis([doc])\n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing {doc_id}: {e}\")\n",
    "        \n",
    "        return self._create_basic_analysis([doc])\n",
    "\n",
    "    def _create_basic_analysis(self, docs: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        \"\"\"Create basic document analysis from metadata\"\"\"\n",
    "        analysis = {}\n",
    "        for doc in docs:\n",
    "            doc_id = doc['id']\n",
    "            \n",
    "            # Handle 'au' as either a list or string\n",
    "            authors = []\n",
    "            if 'au' in doc:\n",
    "                if isinstance(doc['au'], list):\n",
    "                    authors = doc['au']\n",
    "                elif isinstance(doc['au'], str):\n",
    "                    authors = [a.strip() for a in doc['au'].split(';') if a.strip()]\n",
    "            \n",
    "            date = doc.get('dd', '')\n",
    "            title = doc.get('ti', '')\n",
    "            title_terms = [t.lower() for t in title.split() if len(t) > 3] if isinstance(title, str) else []\n",
    "            \n",
    "            analysis[doc_id] = {\n",
    "                \"score\": 5,  # Neutral score\n",
    "                \"entities\": {\n",
    "                    \"people\": authors,\n",
    "                    \"projects\": [],\n",
    "                    \"products\": [],\n",
    "                    \"terms\": title_terms,\n",
    "                    \"dates\": [date] if date else []\n",
    "                }\n",
    "            }\n",
    "        return analysis\n",
    "\n",
    "    def find_surrounding_docs(self, doc_id: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any], List[Dict[str, Any]]]:\n",
    "        \"\"\"Find documents before and after the target document\"\"\"\n",
    "        target_response = requests.get(self.base_url, params={\n",
    "            'q': f'id:{doc_id}',\n",
    "            'wt': 'json',\n",
    "            'fl': 'id,au,ti,bn,dd,dt,availability,pg,attach,access,artifact'\n",
    "        }, verify=False)\n",
    "        \n",
    "        if target_response.status_code != 200:\n",
    "            return [], None, []\n",
    "            \n",
    "        response_docs = target_response.json()['response']['docs']\n",
    "        if not response_docs:\n",
    "            print(f\"Document {doc_id} not found\")\n",
    "            return [], None, []\n",
    "            \n",
    "        target_doc = response_docs[0]\n",
    "        if not self.is_public_doc(target_doc):\n",
    "            return [], None, []\n",
    "            \n",
    "        bates_num = target_doc.get('bn')\n",
    "        if not bates_num:\n",
    "            print(f\"No Bates number for document {doc_id}\")\n",
    "            return [], None, []\n",
    "        \n",
    "        before_params = {\n",
    "            'fq': [\n",
    "                '(collectioncode:\"pm\" OR collectioncode:\"msa\")',\n",
    "                'published:true',\n",
    "                f'bn:[* TO \"{bates_num}\"}}'\n",
    "            ],\n",
    "            'fl': 'id,au,ti,bn,dd,dt,availability,pg,attach,access,artifact',\n",
    "            'wt': 'json',\n",
    "            'rows': '3',\n",
    "            'sort': 'bn_sort desc'\n",
    "        }\n",
    "        \n",
    "        after_params = {\n",
    "            'fq': [\n",
    "                '(collectioncode:\"pm\" OR collectioncode:\"msa\")',\n",
    "                'published:true',\n",
    "                f'bn:{{\"{bates_num}\" TO *]'\n",
    "            ],\n",
    "            'fl': 'id,au,ti,bn,dd,dt,availability,pg,attach,access,artifact',\n",
    "            'wt': 'json',\n",
    "            'rows': '3',\n",
    "            'sort': 'bn_sort asc'\n",
    "        }\n",
    "        \n",
    "        before_url = self.build_url(before_params)\n",
    "        after_url = self.build_url(after_params)\n",
    "        \n",
    "        try:\n",
    "            before_docs = requests.get(before_url, verify=False, timeout=10)\n",
    "            after_docs = requests.get(after_url, verify=False, timeout=10)\n",
    "            \n",
    "            before_results = []\n",
    "            after_results = []\n",
    "            \n",
    "            if before_docs.status_code == 200:\n",
    "                before_results = [doc for doc in before_docs.json()['response']['docs'] \n",
    "                                if self.is_public_doc(doc)]\n",
    "            \n",
    "            if after_docs.status_code == 200:\n",
    "                after_results = [doc for doc in after_docs.json()['response']['docs'] \n",
    "                               if self.is_public_doc(doc)]\n",
    "\n",
    "            return before_results, target_doc, after_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error finding surrounding documents for {doc_id}: {e}\")\n",
    "            return [], None, []\n",
    "\n",
    "    def _get_sorted_documents(self) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Get documents sorted by weighted score\"\"\"\n",
    "        weighted_scores = []\n",
    "        for doc_id, doc_data in self.document_store.items():\n",
    "            if doc_data['analysis']:  # Only include analyzed documents\n",
    "                base_score = doc_data['analysis']['score']\n",
    "                frequency_bonus = min(self.document_frequencies[doc_id] * 0.5, 2.0)\n",
    "                weighted_scores.append((doc_id, base_score + frequency_bonus))\n",
    "        \n",
    "        return sorted(weighted_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    def _prepare_final_results(self) -> Dict[str, Any]:\n",
    "        \"\"\"Prepare final results with all relevant information\"\"\"\n",
    "        sorted_docs = self._get_sorted_documents()\n",
    "        final_results = {}\n",
    "        \n",
    "        for doc_id, weighted_score in sorted_docs:\n",
    "            doc_data = self.document_store[doc_id]\n",
    "            if doc_data['analysis']:  # Only include analyzed documents\n",
    "                final_results[doc_id] = {\n",
    "                    **doc_data['analysis'],\n",
    "                    'frequency': self.document_frequencies[doc_id],\n",
    "                    'weighted_score': weighted_score\n",
    "                }\n",
    "        \n",
    "        return final_results\n",
    "\n",
    "    def analyze_with_gemini(self, docs: List[Dict[Any, Any]], user_query: str, custom_prompt: str = None) -> str:\n",
    "        \"\"\"Generate final analysis using Gemini with simpler, more direct approach\"\"\"\n",
    "        try:\n",
    "            # Create a single temporary directory for full document texts\n",
    "            with tempfile.TemporaryDirectory() as temp_dir:\n",
    "                # Store all document texts in single file with clear separators\n",
    "                analysis_file = os.path.join(temp_dir, \"documents.txt\")\n",
    "                with open(analysis_file, 'w', encoding='utf-8') as f:\n",
    "                    for doc in docs:\n",
    "                        doc_id = doc.get('id')\n",
    "                        title = doc.get('ti', 'No title')\n",
    "                        date = doc.get('dd', 'No date')\n",
    "                        authors = ', '.join(doc.get('au', [])) if isinstance(doc.get('au'), list) else doc.get('au', 'Unknown')\n",
    "                        doc_type = doc.get('dt', 'Unknown type')\n",
    "                        ocr_text = self.document_store[doc_id]['ocr_text'] if doc_id in self.document_store else self.get_ocr_text(doc_id)\n",
    "                        \n",
    "                        f.write(f\"\"\"\n",
    "    === DOCUMENT START ===\n",
    "    ID: {doc_id}\n",
    "    Title: {title}\n",
    "    Date: {date}\n",
    "    Authors: {authors}\n",
    "    Type: {doc_type}\n",
    "\n",
    "    Content:\n",
    "    {ocr_text}\n",
    "\n",
    "    === DOCUMENT END ===\n",
    "\n",
    "    \"\"\")\n",
    "                \n",
    "                # Read all content\n",
    "                with open(analysis_file, 'r', encoding='utf-8') as f:\n",
    "                    all_docs_text = f.read()\n",
    "\n",
    "                # Create single comprehensive prompt\n",
    "                prompt = f\"\"\"Research Question: {user_query}\n",
    "\n",
    "    Analyze these tobacco industry documents. Create a detailed summary for each document in this exact format:\n",
    "\n",
    "    [document_id]: Summary should include:\n",
    "    - Document type and year\n",
    "    - Key findings or main points\n",
    "    - Notable people, organizations, or projects\n",
    "    - Specific data, numbers, or research findings\n",
    "    - Marketing strategies or business decisions\n",
    "    - Public health implications\n",
    "\n",
    "    Example of desired summary format:\n",
    "    [ysvj0228]: This 2008 American Journal of Public Health article examines how tobacco manufacturers manipulated menthol levels in cigarettes to target adolescents and young adults. The authors analyzed internal tobacco industry documents, conducted lab tests on various menthol cigarette brands, and reviewed data from the National Survey on Drug Use and Health. Their findings indicate that lower menthol levels, particularly in brands like Newport and Marlboro Milds, were more appealing to younger smokers because they masked the harshness of cigarettes, facilitating smoking initiation and nicotine addiction. Higher menthol levels were targeted toward long-term smokers. The study also reveals a significant increase in magazine advertising expenditures for menthol brands, despite a decline in overall cigarette sales.\n",
    "\n",
    "    Documents to analyze:\n",
    "\n",
    "    {all_docs_text}\n",
    "\n",
    "    Return ONLY document summaries in the specified format, with one blank line between each summary. Group related documents together if they share common themes or topics.\"\"\"\n",
    "\n",
    "                try:\n",
    "                    response = self.model.generate_content(\n",
    "                        prompt,\n",
    "                        generation_config=genai.GenerationConfig(\n",
    "                            candidate_count=1,\n",
    "                            max_output_tokens=8192,\n",
    "                            temperature=0.2,\n",
    "                            top_k=20,\n",
    "                            top_p=0.8\n",
    "                        )\n",
    "                    )\n",
    "                    return response.text.strip()\n",
    "                    \n",
    "                except ValueError as e:\n",
    "                    if \"reciting from copyrighted material\" in str(e):\n",
    "                        # Return metadata-only summaries\n",
    "                        return \"\\n\\n\".join([\n",
    "                            f\"[{doc.get('id', 'Unknown ID')}]: [Content restricted - Metadata only] \"\n",
    "                            f\"This {doc.get('dd', 'undated')} document titled '{doc.get('ti', 'No title')}' \"\n",
    "                            f\"is a {doc.get('dt', 'document')} authored by \"\n",
    "                            f\"{', '.join(doc.get('au', ['Unknown'])) if isinstance(doc.get('au'), list) else doc.get('au', 'Unknown')}.\"\n",
    "                            for doc in docs\n",
    "                        ])\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in document analysis: {e}\")\n",
    "                    return self._create_basic_metadata_summaries(docs)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error in overall analysis: {e}\")\n",
    "            return self._create_basic_metadata_summaries(docs)\n",
    "\n",
    "    def _create_basic_metadata_summaries(self, docs: List[Dict[str, Any]]) -> str:\n",
    "        \"\"\"Create basic summaries from metadata when analysis fails\"\"\"\n",
    "        return \"\\n\\n\".join([\n",
    "            f\"[{doc.get('id', 'Unknown ID')}]: Basic metadata - \"\n",
    "            f\"This {doc.get('dd', 'undated')} document titled '{doc.get('ti', 'No title')}' \"\n",
    "            f\"is a {doc.get('dt', 'document')} authored by \"\n",
    "            f\"{', '.join(doc.get('au', ['Unknown'])) if isinstance(doc.get('au'), list) else doc.get('au', 'Unknown')}.\"\n",
    "            for doc in docs\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting analysis for: how did malboro market to men?\n",
      "\n",
      "Generated search strategies:\n",
      "\n",
      "Strategy 1:\n",
      "Terms: Marlboro masculinity advertising\n",
      "Filters: {}\n",
      "\n",
      "Strategy 2:\n",
      "Terms: Marlboro men's magazines print ads\n",
      "Filters: {}\n",
      "\n",
      "Strategy 3:\n",
      "Terms: Marlboro cowboy imagery target audience\n",
      "Filters: {}\n",
      "\n",
      "Executing strategy: Marlboro masculinity advertising\n",
      "Skipping document ttcb0036 due to seen title\n",
      "Skipping document ltpl0113 due to seen title\n",
      "Skipping document lxyw0109 due to seen title\n",
      "Skipping document tncy0130 due to seen title\n",
      "Skipping document jhkn0140 due to seen title\n",
      "Skipping document gnwb0112 due to seen title\n",
      "Skipping document qzpf0109 due to seen title\n",
      "Skipping document yyfc0110 due to seen title\n",
      "Skipping document njpl0113 due to seen title\n",
      "Skipping document gtxj0033 due to seen title\n",
      "Skipping document mfyj0129 due to seen title\n",
      "Skipping document hzjc0127 due to seen title\n",
      "Skipping document rfnk0129 due to seen title\n",
      "Skipping document xynf0157 due to seen title\n",
      "Skipping document fsbl0164 due to seen title\n",
      "Skipping document xndw0125 due to seen title\n",
      "Skipping document nhlw0123 due to seen title\n",
      "Skipping document ymmj0129 due to seen title\n",
      "\n",
      "Scores for batch:\n",
      "Document gxby0189: Score 10/10\n",
      "Document sywk0103: Score 9/10\n",
      "Document stjp0013: Score 8/10\n",
      "Document pkgp0180: Score 6/10\n",
      "Document xlfj0013: Score 8/10\n",
      "\n",
      "Scores for batch:\n",
      "Document spvy0105: Score 9/10\n",
      "Document fkjp0042: Score 4/10\n",
      "Document kkxg0099: Score 4/10\n",
      "Document qxhc0118: Score 8/10\n",
      "Document gqjc0110: Score 9/10\n",
      "\n",
      "Scores for batch:\n",
      "Document llcc0110: Score 10/10\n",
      "Document lgbl0022: Score 9/10\n",
      "Document xtpl0113: Score 9/10\n",
      "Document nxcc0110: Score 8/10\n",
      "Document nlwf0138: Score 7/10\n",
      "\n",
      "Scores for batch:\n",
      "Document zqhw0143: Score 8/10\n",
      "Document fypy0139: Score 10/10\n",
      "Document mnfw0172: Score 9/10\n",
      "Document yqgf0127: Score 10/10\n",
      "Document lkhv0035: Score 9/10\n",
      "\n",
      "Scores for batch:\n",
      "Document jxpm0106: Score 9/10\n",
      "Document ytwc0012: Score 9/10\n",
      "Document kkvd0012: Score 8/10\n",
      "Document lxgw0110: Score 7/10\n",
      "Document zzmb0110: Score 8/10\n",
      "\n",
      "Scores for batch:\n",
      "Document jlxy0110: Score 9/10\n",
      "Document yydv0111: Score 10/10\n",
      "Document rfjj0122: Score 8/10\n",
      "Document hhgv0170: Score 10/10\n",
      "Document pfbp0105: Score 9/10\n",
      "\n",
      "Scores for batch:\n",
      "Document tlhh0202: Score 8/10\n",
      "Document mfvj0115: Score 9/10\n",
      "\n",
      "Executing strategy: Marlboro men's magazines print ads\n",
      "Skipping document jshm0070 due to seen title\n",
      "Skipping document sgvf0091 due to seen title\n",
      "Skipping document xzxw0083 due to seen title\n",
      "Skipping document nlbg0091 due to seen title\n",
      "Skipping document tsxw0083 due to seen title\n",
      "\n",
      "Scores for batch:\n",
      "Document ljlj0088: Score 8/10\n",
      "Document gzdb0019: Score 9/10\n",
      "Document xzwk0021: Score 9/10\n",
      "Document xfwb0019: Score 9/10\n",
      "Document jxbb0019: Score 9/10\n",
      "\n",
      "Scores for batch:\n",
      "Document ztfw0059: Score 10/10\n",
      "Document klfx0016: Score 8/10\n",
      "Document zzwj0016: Score 9/10\n",
      "Document msvb0019: Score 8/10\n",
      "Document ssfk0103: Score 9/10\n",
      "\n",
      "Scores for batch:\n",
      "Document jyyb0102: Score 9/10\n",
      "Document gtfk0103: Score 9/10\n",
      "Document zzcm0096: Score 7/10\n",
      "Document gfdm0096: Score 7/10\n",
      "Document qkfn0184: Score 8/10\n",
      "\n",
      "Scores for batch:\n",
      "Document xshm0070: Score 8/10\n",
      "Document rpmy0184: Score 9/10\n",
      "Document hknp0101: Score 8/10\n",
      "Document tsgg0011: Score 7/10\n",
      "Document jffv0019: Score 8/10\n",
      "\n",
      "Scores for batch:\n",
      "Document pjmp0013: Score 8/10\n",
      "Document ssgg0011: Score 8/10\n",
      "Document zygv0014: Score 7/10\n",
      "Document kjxl0094: Score 7/10\n",
      "Document lkyg0016: Score 6/10\n",
      "\n",
      "Scores for batch:\n",
      "Document rhvm0096: Score 8/10\n",
      "Document skbb0014: Score 2/10\n",
      "Document rkbb0014: Score 2/10\n",
      "Document xmpc0099: Score 3/10\n",
      "Document hxvd0185: Score 4/10\n",
      "\n",
      "Scores for batch:\n",
      "Document yjfw0091: Score 8/10\n",
      "Document tjcf0083: Score 8/10\n",
      "Document tfdm0096: Score 9/10\n",
      "Document zmfw0091: Score 9/10\n",
      "Document gxdm0096: Score 9/10\n",
      "\n",
      "Scores for batch:\n",
      "Document srdm0096: Score 8/10\n",
      "Document jzwk0021: Score 9/10\n",
      "Document hlwl0088: Score 9/10\n",
      "Document xygn0056: Score 7/10\n",
      "Document lrjv0019: Score 8/10\n",
      "\n",
      "Scores for batch:\n",
      "Document xjwx0118: Score 2/10\n",
      "Document yggn0056: Score 8/10\n",
      "Document srbf0105: Score 9/10\n",
      "Document lfwx0061: Score 1/10\n",
      "\n",
      "Executing strategy: Marlboro cowboy imagery target audience\n",
      "Skipping document fxkc0118 due to seen title\n",
      "Skipping document hsvd0110 due to seen title\n",
      "Skipping document ggyp0110 due to seen title\n",
      "Skipping document pmfm0113 due to seen title\n",
      "Skipping document hrcg0124 due to seen title\n",
      "Skipping document lspd0110 due to seen title\n",
      "Skipping document ftff0109 due to seen title\n",
      "Skipping document mjyp0110 due to seen title\n",
      "Skipping document zfdc0112 due to seen title\n",
      "Skipping document hrwn0122 due to seen title\n",
      "Skipping document zmnb0110 due to seen title\n",
      "Skipping document zyfc0110 due to seen title\n",
      "Skipping document mfhc0110 due to seen title\n",
      "Skipping document kfnb0110 due to seen title\n",
      "Skipping document rnjl0113 due to seen title\n",
      "Skipping document xyfv0111 due to seen title\n",
      "Skipping document sfhc0110 due to seen title\n",
      "Skipping document rsbd0116 due to seen title\n",
      "Skipping document rxwm0113 due to seen title\n",
      "Skipping document gkhm0112 due to seen title\n",
      "Skipping document txxd0110 due to seen title\n",
      "Skipping document gqkb0101 due to seen title\n",
      "Skipping document zscx0096 due to seen title\n",
      "Skipping document gtcx0096 due to seen title\n",
      "Skipping document mfxv0101 due to seen title\n",
      "Skipping document nzvw0094 due to seen title\n",
      "Skipping document gzyd0094 due to seen title\n",
      "Skipping document hnld0094 due to seen title\n",
      "Skipping document nxln0088 due to seen title\n",
      "Skipping document yynx0091 due to seen title\n",
      "Skipping document szmy0100 due to seen title\n",
      "\n",
      "Scores for batch:\n",
      "Document txkj0191: Score 8/10\n",
      "Document xxyw0189: Score 7/10\n",
      "Document mtvn0190: Score 9/10\n",
      "Document fxjw0180: Score 6/10\n",
      "Document qzmj0191: Score 2/10\n",
      "\n",
      "Scores for batch:\n",
      "Document yhyl0190: Score 8/10\n",
      "Document mrgn0059: Score 9/10\n",
      "Document tppd0110: Score 10/10\n",
      "Document pjhd0110: Score 10/10\n",
      "Document sndv0111: Score 10/10\n",
      "\n",
      "Scores for batch:\n",
      "Document nlxx0149: Score 10/10\n",
      "Document zrhv0099: Score 8/10\n",
      "Document fmfd0096: Score 8/10\n",
      "Document jnlb0099: Score 7/10\n",
      "Document hqlv0101: Score 9/10\n",
      "\n",
      "Scores for batch:\n",
      "Document snxl0000: Score 9/10\n",
      "Document nykb0223: Score 3/10\n",
      "Document hgbg0102: Score 7/10\n",
      "Document zfjy0096: Score 8/10\n",
      "\n",
      "Expansion iteration 1\n",
      "\n",
      "Scores for batch:\n",
      "Document fxby0189: Score 3/10\n",
      "Document zhby0189: Score 8/10\n",
      "Document thby0189: Score 7/10\n",
      "Document hxby0189: Score 9/10\n",
      "Document xxby0189: Score 2/10\n",
      "\n",
      "Scores for batch:\n",
      "Document rrhm0166: Score 0/10\n",
      "Document klcc0110: Score 10/10\n",
      "Document qlcc0110: Score 10/10\n",
      "Document mlcc0110: Score 0/10\n",
      "Document jnmd0116: Score 8/10\n",
      "Skipping document hypy0139 due to seen title\n",
      "Skipping document yhmg0134 due to seen title\n",
      "\n",
      "Scores for batch:\n",
      "Document nhxn0099: Score 1/10\n",
      "Document znpy0139: Score 2/10\n",
      "Document tnpy0139: Score 7/10\n",
      "Document gypy0139: Score 3/10\n",
      "Skipping document lxpk0033 due to seen title\n",
      "\n",
      "Scores for batch:\n",
      "Document nqgf0127: Score 8/10\n",
      "Document mxpk0033: Score 2/10\n",
      "Document pqgf0127: Score 9/10\n",
      "Document kqkj0106: Score 7/10\n",
      "Document lqkj0106: Score 10/10\n",
      "\n",
      "Scores for batch:\n",
      "Document nydv0111: Score 9/10\n",
      "Document srfj0115: Score 8/10\n",
      "Document rrfj0115: Score 6/10\n",
      "Document pydv0111: Score 8/10\n",
      "Document qydv0111: Score 7/10\n",
      "Skipping document xsjp0169 due to seen title\n",
      "\n",
      "Scores for batch:\n",
      "Document hgpg0069: Score 1/10\n",
      "Document szmx0218: Score 2/10\n",
      "Document ysyk0006: Score 8/10\n",
      "Document tplp0169: Score 7/10\n",
      "\n",
      "Scores for batch:\n",
      "Document fzfw0059: Score 9/10\n",
      "Document gzfw0059: Score 8/10\n",
      "Document nylj0092: Score 9/10\n",
      "Document ttfw0059: Score 1/10\n",
      "Document yylj0092: Score 9/10\n",
      "Skipping document xqpd0110 due to seen title\n",
      "\n",
      "Scores for batch:\n",
      "Document sppd0110: Score 9/10\n",
      "Document hqpd0110: Score 8/10\n",
      "Document zppd0110: Score 10/10\n",
      "Document mlyd0110: Score 0/10\n",
      "Document rlyd0110: Score 2/10\n",
      "Skipping document qjhd0110 due to seen title\n",
      "Skipping document fnxd0110 due to seen title\n",
      "Skipping document gnxd0110 due to seen title\n",
      "\n",
      "Scores for batch:\n",
      "Document yjhd0110: Score 0/10\n",
      "Document pzkv0111: Score 3/10\n",
      "Document yzkv0111: Score 7/10\n",
      "Skipping document tndv0111 due to seen title\n",
      "\n",
      "Scores for batch:\n",
      "Document rndv0111: Score 10/10\n",
      "Document qzml0112: Score 1/10\n",
      "Document pzml0112: Score 1/10\n",
      "Document pzfj0115: Score 9/10\n",
      "Document qzfj0115: Score 9/10\n",
      "Skipping document prxm0138 due to seen title\n",
      "\n",
      "Scores for batch:\n",
      "Document rrhg0189: Score 8/10\n",
      "Document qrhg0189: Score 7/10\n",
      "Document yrhg0189: Score 7/10\n",
      "Document nrxm0138: Score 2/10\n",
      "Document yrxm0138: Score 0/10\n",
      "\n",
      "Scores for batch:\n",
      "Document hknw0077: Score 9/10\n",
      "Document tlbf0078: Score 9/10\n",
      "Document rywk0103: Score 2/10\n",
      "Document llnw0077: Score 8/10\n",
      "Document mlnw0077: Score 8/10\n",
      "\n",
      "Scores for batch:\n",
      "Document mgmy0038: Score 10/10\n",
      "Document lgmy0038: Score 9/10\n",
      "Document rpvy0105: Score 9/10\n",
      "Document ngmy0038: Score 9/10\n",
      "Document ygmy0038: Score 9/10\n",
      "\n",
      "Scores for batch:\n",
      "Document fqjc0110: Score 10/10\n",
      "Document zpjc0110: Score 9/10\n",
      "Document jmgd0116: Score 10/10\n",
      "Document hqjc0110: Score 9/10\n",
      "Document ypjc0110: Score 8/10\n",
      "Skipping document ttcb0036 due to seen title\n",
      "\n",
      "Scores for batch:\n",
      "Document yhpw0033: Score 0/10\n",
      "Document nqdc0036: Score 0/10\n",
      "Document sffm0030: Score 8/10\n",
      "Document fkcx0025: Score 0/10\n",
      "Document lgcj0038: Score 0/10\n",
      "Found 78 new documents\n",
      "Total unique documents: 173\n",
      "\n",
      "Expansion iteration 2\n",
      "Skipping document hypy0139 due to seen title\n",
      "Skipping document yhmg0134 due to seen title\n",
      "Skipping document lxpk0033 due to seen title\n",
      "Skipping document xsjp0169 due to seen title\n",
      "Skipping document xqpd0110 due to seen title\n",
      "Skipping document qjhd0110 due to seen title\n",
      "Skipping document fnxd0110 due to seen title\n",
      "Skipping document gnxd0110 due to seen title\n",
      "Skipping document tndv0111 due to seen title\n",
      "Skipping document prxm0138 due to seen title\n",
      "\n",
      "Scores for batch:\n",
      "Document plcc0110: Score 9/10\n",
      "Document ylcc0110: Score 1/10\n",
      "\n",
      "Scores for batch:\n",
      "Document mqkj0106: Score 9/10\n",
      "Document nqkj0106: Score 8/10\n",
      "Document yqkj0106: Score 7/10\n",
      "Skipping document xqpd0110 due to seen title\n",
      "Found 7 new documents\n",
      "Total unique documents: 180\n",
      "\n",
      "Expansion iteration 3\n",
      "Skipping document lxpk0033 due to seen title\n",
      "Skipping document xqpd0110 due to seen title\n",
      "Skipping document xqpd0110 due to seen title\n",
      "Skipping document tndv0111 due to seen title\n",
      "Skipping document hypy0139 due to seen title\n",
      "Skipping document yhmg0134 due to seen title\n",
      "Skipping document xsjp0169 due to seen title\n",
      "Skipping document qjhd0110 due to seen title\n",
      "Skipping document fnxd0110 due to seen title\n",
      "Skipping document gnxd0110 due to seen title\n",
      "Skipping document tndv0111 due to seen title\n",
      "Found 1 new documents\n",
      "Total unique documents: 181\n",
      "Reached saturation\n",
      "\n",
      "Final Analysis:\n",
      "**Documents related to Marlboro's Marketing to Men (1990s):**\n",
      "\n",
      "[llcc0110]: This undated market research report from the Leo Burnett Agency details in-depth interviews (36 one-hour interviews: 13 men aged 18-30, 10 men aged 30-45, and 13 women) conducted to understand the appeal of new Marlboro advertising.  Key findings for men focused on positive responses to ads portraying aspects of \"Cowboy Life\" (where he lives, what he does to relax, his work, his personality).  Positive responses were linked to new information, dimensionalization of masculinity, realism, and aspirational yet approachable imagery.  Negative reactions were associated with contrived or sexist depictions.  The report also analyzed the effectiveness of various copy lines, finding that lines evoking freedom and independence resonated strongly, while others were perceived as inappropriate or confusing.  The study highlights the need to incorporate diverse aspects of cowboy life and avoid stereotypical portrayals of masculinity.  Public health implications are not directly addressed, but the research informs marketing strategies that could influence smoking initiation and continuation.\n",
      "\n",
      "\n",
      "[klcc0110]: This November 1991 advertisement/brand review details the launch of Marlboro Medium, a low-tar cigarette extension.  The key finding is the successful market penetration (1.7% share in four months) achieved through a marketing campaign emphasizing masculine imagery consistent with, yet distinct from, Marlboro Red.  The advertising used \"Cowboy tack\" and the tagline \"A new low tar cigarette. When you want more flavor.\"  The packaging was similar to Marlboro Red but with a lighter look.  The product itself aimed to bridge the flavor gap between Marlboro Red and Lights.  The launch involved significant media spending ($63.6 million in three months), direct marketing (mailings with coupons and incentives), and broad distribution (85%).  High aided brand awareness (66% overall, 92% among 18-24 year olds) was achieved.  The document highlights the strategic decision to address the \"Reds downswitcher\" problem and compete with Camel. Public health implications are indirect, focusing on market share and consumer preference for lower-tar options.\n",
      "\n",
      "\n",
      "[yqgf0127]: This March 1993 report analyzes Marlboro USA advertising research.  The key finding is that the Marlboro campaign had lost impact due to its narrow focus on the cowboy icon.  However, the research confirmed the continued relevance of Western frontier values and the Marlboro Country campaign, provided it dimensionalized cowboy life and the \"fantasy\" of the West.  Key elements for maintaining emotional context included new information, dimensionalization of masculinity, unique execution, realism, and an aspirational yet approachable cowboy.  The report outlines a future direction for the campaign, emphasizing storytelling, authenticity, timelessness, and a broader portrayal of masculinity.  Marlboro values (freedom, opportunity, challenge, self-sufficiency, respect for nature, mastery of destiny) are explicitly listed.  The public health implications are implicit, suggesting that a more nuanced and appealing campaign could influence smoking behavior.\n",
      "\n",
      "\n",
      "[tppd0110]: This 1993 brand plan from the Marlboro Worldwide Creative Review Committee addresses the changing attitudinal values of young adult male smokers.  Key findings highlight a shift away from materialism and a reinterpretation of masculinity, with independence and freedom remaining central.  The report identifies diminished media exposure of cowboys and the West, and a redefinition of masculinity as key brand issues.  The creative assignment focuses on dimensionalizing the cowboy and Marlboro Country, creating a broader range of ads, and achieving a fresher, more aspirational image.  The document also discusses guidelines for Marlboro Lights and Menthol, emphasizing the need for distinct yet consistent imagery.  Public health implications are not explicitly stated but are inherent in the discussion of marketing strategies aimed at maintaining and expanding market share among young adult smokers.\n",
      "\n",
      "\n",
      "[lqkj0106]: This March 1993 report outlines a Marlboro creative brief.  The key issue is maintaining advertising relevancy among young adult male smokers (YAMS) in mature markets, where price gaps and competitive image brands pose challenges.  The report addresses diminished media exposure of the cowboy myth and the redefinition of masculinity.  The creative assignment focuses on dimensionalizing the cowboy and Marlboro Country, creating diverse ads, and achieving a fresher, more vital image.  The advertising objective is to reinforce Marlboro's position as the brand of choice and increase likability and contemporary appeal.  The document mentions ongoing research into new creative directions (\"untold stories\").  Public health implications are implicit, focusing on the need to maintain market share among a key demographic.\n",
      "\n",
      "\n",
      "[zppd0110]: This 1993 brand plan from the Marlboro Worldwide Creative Review Committee provides creative briefs for print, TV, and radio advertising.  The key issue is the need to generate principal Marlboro advertising due to reduced output from PM USA.  The plan emphasizes story value, diverse subject areas (vistas, action, portraits, camaraderie), and a more natural and heroic portrayal of the cowboy.  It addresses regional variations in desired tonality and style, and the need for smoking/non-smoking versions of commercials.  The document also highlights the importance of music (\"Magnificent Seven\") and addresses specific regional considerations.  Public health implications are implicit, focusing on the need to maintain and enhance brand appeal to young adult male smokers.\n",
      "\n",
      "\n",
      "[rndv0111]: This 1993 report summarizes problems and challenges identified at a Monte Carlo meeting.  Key issues include the declining full-flavor segment, pressure from low-tar brands, and the need to maintain Marlboro's appeal to young adult smokers (YAS).  Regional challenges are discussed, including price gaps, image appeal, and the need for innovative promotions.  The report highlights the need for improved internal communication and consistent branding.  The document emphasizes the importance of addressing price issues, broadening the Marlboro Country campaign, and resolving issues related to Marlboro Lights' brand personality.  Public health implications are implicit, focusing on the need to maintain market share in a competitive and evolving market.\n",
      "\n",
      "\n",
      "[sndv0111]: This 1993 document from the Marlboro Worldwide Creative Review Committee mirrors the content of [tppd0110], focusing on the changing attitudinal values of young adult male smokers and the need to adapt Marlboro's advertising.  The key issues are the same: diminished media exposure of the cowboy myth, redefinition of masculinity, and the need to dimensionalize the cowboy and Marlboro Country.  The creative assignment, advertising objectives, and executional details are nearly identical to [tppd0110].  The document also includes similar discussions of Marlboro Lights, Menthol, and Medium.  Public health implications are implicit, focusing on the need to maintain and expand market share among young adult smokers.\n",
      "\n",
      "\n",
      "[qlcc0110]: This document is simply a file folder label indicating the contents relate to Marlboro Medium's U.S. story in November 1991.  It provides no substantive information for analysis.\n",
      "\n",
      "\n",
      "[pqgf0127]: This March 25, 1993 memo outlines a meeting agenda between Philip Morris and Bill Murray.  The memo discusses the plan to address Marlboro Red's decline, contemporize advertising, innovate promotions, and coordinate efforts between International and PMUSA.  It mentions parallel research results and the need to link research to the advertising plan.  The memo also proposes focusing on linking research results to the advertising plan, addressing Bill Murray's previous questions regarding Camel's advertising, and exposing the concept of US Originals.  Further research on Marlboro Lights versus Red is suggested.  Public health implications are not directly addressed, but the memo highlights the need to adapt marketing strategies to address declining market share.\n",
      "\n",
      "Total documents analyzed: 171\n",
      "Total documents seen: 181\n"
     ]
    }
   ],
   "source": [
    "analyzer = TobaccoDocAnalyzer()\n",
    "    \n",
    "query = input(\"Enter your research question about tobacco documents: \")\n",
    "results = analyzer.analyze_topic(query)\n",
    "\n",
    "print(\"\\nFinal Analysis:\")\n",
    "print(results['final_analysis'])\n",
    "print(f\"\\nTotal documents analyzed: {results['document_count']}\")\n",
    "print(f\"Total documents seen: {results['total_documents_seen']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19638 total documents\n",
      "\n",
      "Document:\n",
      "Title: No title\n",
      "Date: 1999 August 18\n",
      "Type: ['memo']\n",
      "Collection: ['Philip Morris Records', 'Master Settlement Agreement']\n",
      "\n",
      "Document:\n",
      "Title: SPECIAL REPORT TO THE FTC\n",
      "Date: 1993 July 06\n",
      "Type: ['memo']\n",
      "Collection: ['Philip Morris Records', 'Master Settlement Agreement']\n",
      "\n",
      "Document:\n",
      "Title: FTC SPECIAL REPORT FOR 19960000\n",
      "Date: 1997 April 22\n",
      "Type: ['memo']\n",
      "Collection: ['Philip Morris Records', 'Master Settlement Agreement']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "base_url = \"https://solr.idl.ucsf.edu/solr/ltdl3/select\"\n",
    "\n",
    "params = {\n",
    "    'q': 'menthol',  \n",
    "    'fq': [  \n",
    "        'availability:public',\n",
    "        'dd:[19900101 TO 19991231]',\n",
    "        'dt:memo',\n",
    "        'collection:\"Philip Morris\"'\n",
    "    ],\n",
    "    'wt': 'json',\n",
    "    'rows': 10     \n",
    "}\n",
    "\n",
    "response = requests.get(base_url, params=params, verify=False)\n",
    "results = response.json()\n",
    "print(f\"Found {results['response']['numFound']} total documents\")\n",
    "\n",
    "# Look at documents including their type\n",
    "if results['response']['numFound'] > 0:\n",
    "    for doc in results['response']['docs'][:3]:\n",
    "        print(\"\\nDocument:\")\n",
    "        print(f\"Title: {doc.get('ti', 'No title')}\")\n",
    "        print(f\"Date: {doc.get('dd', 'No date')}\")\n",
    "        print(f\"Type: {doc.get('dt', 'No type')}\")\n",
    "        print(f\"Collection: {doc.get('collection', 'No collection')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "38cca0c38332a56087b24af0bc80247f4fced29cb4f7f437d91dc159adec9c4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
